<!DOCTYPE html>
<html>
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.2.0">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <title>GPT-3</title>
  <link rel="stylesheet" type="text/css" href="../../../style.css">
  <link rel="alternate" type="application/atom+xml" href="https://blog.rfox.eu/atom_cz.xml">
  <link rel="shortcut icon" href="/favicon.ico">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@Bystroushaak">
  <meta name="twitter:creator" content="@Bystroushaak">
  <meta name="twitter:title" content="GPT-3">
  <meta name="twitter:description" content="PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno pÅ™edstavenÃ½ druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­.">
  <meta name="twitter:image" content="https://blog.rfox.eu/cz/Predstaveni/GPT-3/word2vec_examples_thumb.jpg">
  <script src="../../../scripts.js">
  </script>
  <meta name="description" content="PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno pÅ™edstavenÃ½ druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­."><!-- Global site tag (gtag.js) - Google Analytics -->

  <script src="https://www.googletagmanager.com/gtag/js?id=UA-142545439-1">
  </script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'UA-142545439-1');
  </script>
</head>
<body onload="on_body_load();">
  <div id="sidebar_top">
    <span><a href="https://blog.rfox.eu/atom_cz.xml"><img style="width: 3em;" src="../../../rss_icon.png"></a> &nbsp; &nbsp; <a href="https://twitter.com/Bystroushaak"><img style="width: 3em;" src="../../../twitter_icon.png"></a></span>
    <div id="last_five_top">
      <h3>New posts</h3>
      <ul>
        <li>
          <a href="../GPT-3.html" title="GPT-3">GPT-3</a>
        </li>
        <li>
          <a href="https://blog.rfox.eu/atom_cz.xml">VytvoÅ™en novÃ½ Atom (RSS) feed pro Äeskou sekci</a>
        </li>
      </ul>
    </div>
    <div id="links_from_other_pages">
      <h3>Links to this page:</h3>
      <ul>
        <li>
          <a href="../../../Zmeny.html" title="ZmÄ›ny">ZmÄ›ny</a>
        </li>
        <li>
          <a href="../../../Zmeny/Poslednich_nekolik_tydnu_rozechviva_vlny_mych_socialnich_siti_fenomen.html" title="PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno pÅ™edstavenÃ½ druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­.">PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno pÅ™edstavenÃ½ druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­.</a>
        </li>
        <li>
          <a href="../../../en/Weekly_updates/Newsletter_2020-09-12_Waves_of_productivity.html" title="Newsletter 2020-09-12; Waves of productivity">Newsletter 2020-09-12; Waves of productivity</a>
        </li>
      </ul>
    </div>
    <div>
      <h3>Blog categories</h3>
      <ul class="no_icon">
        <li>
          <a href="../../Programovani.html" title="ProgramovÃ¡nÃ­">ğŸ“‚ ProgramovÃ¡nÃ­</a>
        </li>
        <li>
          <a href="../../Knihy.html" title="Knihy">ğŸ“‚ Knihy</a>
        </li>
        <li>
          <a href="../../Predstaveni.html" title="PÅ™edstavenÃ­">ğŸ“‚ PÅ™edstavenÃ­</a>
        </li>
        <li>
          <a href="../../Povidky.html" title="PovÃ­dky">ğŸ“‚ PovÃ­dky</a>
        </li>
        <li>
          <a href="../../Hrbitov.html" title="HÅ™bitov">ğŸ“‚ HÅ™bitov</a>
        </li>
        <li>
          <a href="../../Ostatni.html" title="OstatnÃ­">ğŸ“‚ OstatnÃ­</a>
        </li>
        <li>
          <a href="../../Abclinuxu.html" title="Abclinuxu">ğŸ“‚ Abclinuxu</a>
        </li>
        <li>
          <a href="../../index.html" title="Czech section">ğŸ“‚ Czech section</a>
        </li>
      </ul>
    </div>
  </div><a class="breadcrumb" href="../../../index.html" title="Bystroushaak's blog">Bystroushaak's blog</a> / <a class="breadcrumb" href="../../index.html" title="Czech section">Czech section</a> / <a class="breadcrumb" href="../index.html" title="PÅ™edstavenÃ­">PÅ™edstavenÃ­</a> / GPT-3
  <article id="1c47af32-697d-4567-b18c-a95f430c3386" class="page sans">
    <header>
      <h1 class="page-title">GPT-3</h1>
    </header>
    <div class="page-body">
      <p id="a46dc6e0-7a1d-4c35-954f-b1b786716aaa" class=""><time>@2020/08/19</time></p>
      <p id="104a2dba-8ada-4f74-8a41-eafb02c49cac" class="">PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno <a href="https://arxiv.org/pdf/2005.14165.pdf">pÅ™edstavenÃ½</a> druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­.</p>
      <p id="838536f1-60e0-47f8-9812-cc7b23a46982" class="">NÃ¡sledujÃ­cÃ­ obrÃ¡zek pÄ›knÄ› ukazuje rozdÃ­l v poÄtu parametrÅ¯ oproti pÅ™edchozÃ­m modelÅ¯m:</p>
      <figure id="c900aedc-af6a-42cc-a9b6-41d2af9f8b04" class="image">
        <a href="Untitled.png" title="Untitled.png"><img style="width:1700px" src="Untitled_thumb.jpg"></a>
        <figcaption>
          <em>(UkÃ¡zka postupu vÃ½voje poÄtu parametrÅ¯ neuronovÃ©ho modelu transformerÅ¯ v Äase. Zdroj:</em> <em><a href="https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/">Why GPT-3 matters</a></em><em>.)</em>
        </figcaption>
      </figure>
      <h1 id="03da4b9b-2983-4030-957b-a132ad59f6f1" class="">UkÃ¡zky</h1>
      <p id="69eb8cba-3f5d-422c-80cc-4eb0c71994f7" class="">PojÄme se prvnÄ› podÃ­vat na ty zajÃ­mavÄ›jÅ¡Ã­ pÅ™Ã­klady pouÅ¾itÃ­ GPT-3.</p>
      <h2 id="39d3af09-7c59-43eb-bc6a-2dc6eb8da2fa" class="">GenerovÃ¡nÃ­ textÅ¯</h2>
      <p id="fe3a49ae-4d0f-4eda-b5a1-849c5a2239fa" class="">Asi nepÅ™ekvapÃ­, Å¾e GPT-3 je schopnÃ© na zÃ¡kladÄ› krÃ¡tkÃ©ho promptu, kterÃ½ uvede tÃ©ma textu, napsat pÅ™Ã­bÄ›h, ÄlÃ¡nek, blog, nebo semestrÃ¡lnÃ­ prÃ¡ci na zadanÃ© tÃ©ma.</p>
      <p id="67e2ee55-ffc1-42dd-8a2f-d0717f1114e9" class="">PodobnÃ© projekty zde byly uÅ¾ dÅ™Ã­v, a byly napÅ™Ã­klad schopny dogenerovat dalÅ¡Ã­ odstavce podobnÃ© pÅ¯vodnÃ­mu. Za vÅ¡echny napÅ™Ã­klad GPT-2 a <a href="https://talktotransformer.com/">talktotransformer</a>. SÃ¡m jsem kdysi zkouÅ¡el Markovovo modely a rekurentnÃ­ neuronovÃ© sÃ­tÄ›, kterÃ© pÅ™estoÅ¾e ve srovnÃ¡nÃ­ s GPT-3 pÅ¯sobÃ­ jako hraÄka, takÃ© zvlÃ¡dly generovat zajÃ­mavÃ© vÃ½sledky.</p>
      <p id="3df2f13a-ddd5-4805-8277-a380278c6056" class="">Gwern na tohle tÃ©ma sepsal celÃ½ blogpost: <a href="https://www.gwern.net/GPT-3">https://www.gwern.net/GPT-3</a></p>
      <h2 id="2860d0ff-05e6-4d66-9b5d-d86f9b273892" class="">Aritmetika</h2>
      <p id="fbc84656-1bd6-425a-98a2-1103c361d01b" class="">NÄ›jak se stalo, Å¾e GPT-3 se na mnoÅ¾inÄ› dat nauÄilo sÄÃ­tat a nÃ¡sobit ÄÃ­sla. MÃ¡ sice problÃ©my s vÄ›tÅ¡Ã­mi ÄÃ­sly, obÄas dÄ›lÃ¡ chyby, ale i tak je to fascinujÃ­cÃ­, kdyÅ¾ si uvÄ›domÃ­te, Å¾e mu nikdo nevysvÄ›tloval koncept ÄÃ­sel, ani aritmetickÃ½ch operacÃ­. PÅ™edstavte si, Å¾e by vÃ¡s nikdo neuÄil ÄÃ­st a vy se nauÄili pouÅ¾Ã­vat aritmetiku na zÃ¡kladÄ› ÄtenÃ­ knih v ÄÃ­nÅ¡tinÄ›, kterou vÃ¡s taky nikdo nenauÄÃ­.</p>
      <p id="8e6346c4-03e8-40c9-86f1-b9dc0b39012a" class="">ZajÃ­malo by mÄ› kam by se model dostal, kdyby mu nÄ›kdo v trÃ©novacÃ­ch datech nacpal spoustu aritmetiky a matematiky.</p>
      <h2 id="7cc3e628-ed93-4729-9888-57da2d55d872" class="">Layout generator</h2>
      <p id="4ffd7b99-fbd6-4132-97fb-16b5d91de231" class="">PrvnÃ­ ukÃ¡zka specializovanÃ©ho pouÅ¾itÃ­, kde jsem se zarazil a doÅ¡lo mi, Å¾e tohle nebude hraÄka jako GPT-2, byl tweet, kde <a href="https://twitter.com/sharifshameem/status/1282676454690451457">Sharif Shameem</a> napojil svÅ¯j projekt na GPT-3 API a â€pÅ™edpÅ™ipravilâ€œ ho trochou ukÃ¡zek CSS.</p>
      <p id="cc840fa1-3b44-43fe-a0cd-7ed69833520f" class="">â€PÅ™edpÅ™ipravenÃ­mâ€œ zde nenÃ­ myÅ¡leno trÃ©novÃ¡nÃ­, ale jen uvedenÃ­ kontextu pÅ™edtÃ­m, neÅ¾ je modelu pÅ™edÃ¡n vÃ¡Å¡ text. V tomhle pÅ™Ã­padÄ› bylo modelu ukÃ¡zÃ¡no trochu CSS a on najednou zvlÃ¡dl generovat layout podle textovÃ©ho popisu.</p>
      <figure id="6e7658e6-5c3d-46ac-8ec1-7dc4e6918013" class="image">
        <a href="a_button_that_looks_like_watermelon.png" title="a_button_that_looks_like_watermelon.png"><img style="width:938px" src="a_button_that_looks_like_watermelon_thumb.jpg"></a>
      </figure>
      <p id="91c24b7e-f643-4500-8713-c72e471df199" class="">Nejen Å¾e GPT-3 pochopil co se po nÄ›m chce, ale navÃ­c zvlÃ¡dl vygenerovat i patÅ™iÄnÃ½ HTML a CSS kÃ³d. Pokud vÃ¡m to zatÃ­m nepÅ™iÅ¡lo, tak tohle je opravdu k zamyÅ¡lenÃ­.</p>
      <div id="a6dc2a97-db72-4acd-8336-82f5d0011592" class="column-list">
        <div id="dec3290f-1b86-4519-a2ac-648c578061c6" style="width:50%" class="column">
          <figure id="c12b2fda-5b57-43ca-af25-97e7dc7ba1c9" class="image">
            <a href="subscribe_button_example.jpg" title="subscribe_button_example.jpg"><img style="width:906px" src="subscribe_button_example_thumb.jpg"></a>
          </figure>
        </div>
        <div id="2a4933fe-42c6-4893-846f-10a4c2722eab" style="width:50%" class="column">
          <figure id="6aaa1431-3409-4655-9921-6ade61ab9d3b" class="image">
            <a href="colors.png" title="colors.png"><img style="width:938px" src="colors_thumb.jpg"></a>
          </figure>
        </div>
      </div>
      <p id="e1b325fd-0034-4310-b8f7-5e502ff6d3d8" class=""></p>
      <div id="dd60ece9-3281-40cc-9a73-10ecedf2e121" class="column-list">
        <div id="c7dce912-c73d-47f6-ac23-63674b49801b" style="width:50%" class="column">
          <figure id="e1c08239-553d-4d2e-95a2-4612d18c4d31" class="image">
            <a href="trumps_hair_example.png" title="trumps_hair_example.png"><img style="width:938px" src="trumps_hair_example_thumb.jpg"></a>
          </figure>
        </div>
        <div id="d2070cb6-9f15-4814-8389-5fdc098a2619" style="width:50%" class="column">
          <figure id="f7726abd-8698-436e-8608-ee28d9801de8" class="image">
            <a href="mpv-shot0013.png" title="mpv-shot0013.png"><img style="width:938px" src="mpv-shot0013_thumb.jpg"></a>
          </figure>
        </div>
      </div>
      <p id="bb129ff9-4485-44e1-bead-8102d1d86c94" class="">Video zahrnuje nÄ›kolik ukÃ¡zek, nÄ›kterÃ© fungujÃ­ vÃ­ce, jinÃ© mÃ©nÄ›, dokonce je tam i syntatickÃ¡ chyba. Ale vzhledem k tomu Å¾e model nikdo neuÄil pouÅ¾Ã­vat HTML, nebo kÃ³dovat CSS, tak se jednÃ¡ o fantastickÃ© vÃ½sledky. Tohle vÅ¡echno pochytil z nÃ¡hodnÃ½ch textÅ¯. Kdyby byl trÃ©novanÃ½ speciÃ¡lnÄ› na HTML / CSS, tak se ÃºÄinnost jistÄ› podstatnÄ› zvÃ½Å¡Ã­.</p>
      <p id="3513845f-d9d9-4a34-b24a-791dd8ae3896" class="">Sharif pozdÄ›ji pÅ™idal jeÅ¡tÄ› dalÅ¡Ã­ ukÃ¡zku, kde GPT-3 vytvÃ¡Å™Ã­ aplikace v reactu i s funkÄnÃ­m kÃ³dem, kde je nÄ›kolik tlaÄÃ­tek updatujÃ­cÃ­ch data na backendu:</p>
      <ul id="9cf3ba4d-4dc9-4699-9dcc-5102e4f934bd" class="bulleted-list">
        <li>
          <a href="https://twitter.com/sharifshameem/status/1284095222939451393">https://twitter.com/sharifshameem/status/1284095222939451393</a>
        </li>
      </ul>
      <h2 id="39acef8c-f87a-406c-bbce-8a8d2713dbd6" class="">Terapie</h2>
      <p id="57ed5d77-c9bf-49c6-a758-fe91c9a9904d" class="">Nick Cammarata <a href="https://twitter.com/nicklovescode/status/1283326066338062337">zkouÅ¡el</a> pouÅ¾Ã­vat GPT-3 jako terapeuta. PrvnÃ­ Å™Ã¡dka, kterou je moÅ¾nÃ© na obrÃ¡zku vidÄ›t je ono jiÅ¾ zmiÅˆovanÃ© â€pÅ™edpÅ™ipravenÃ­â€œ, kterÃ© dÃ¡ GPT-3 kontext rozhovoru. V rozhovoru pak GPT-3 vystupuje jako John.</p>
      <figure id="c98cb99d-d067-47ad-99ae-eea0a2324746" class="image">
        <a href="Untitled_1.png" title="Untitled_1.png"><img style="width:528px" src="Untitled_1_thumb.jpg"></a>
        <figcaption>
          <em>(Zdroj obrÃ¡zku:</em> <em><a href="https://twitter.com/nicklovescode/status/1283326066338062337">https://twitter.com/nicklovescode/status/1283326066338062337</a></em><em>)</em>
        </figcaption>
      </figure>
      <h2 id="916a3b5b-8a00-496a-8f00-f715cdd5a88b" class="">GPT-3 jako doktor</h2>
      <p id="abe71dc1-86f4-4e74-820a-a22b82abf2d8" class=""><a href="https://twitter.com/QasimMunye/status/1278750809094750211">Quasima Munye napadlo</a> poloÅ¾it GPT-3 otÃ¡zku z lÃ©kaÅ™skÃ©ho oboru:</p>
      <figure id="e3b0cbc6-84eb-47b7-b0db-cc36336bdab4" class="image">
        <a href="Untitled_2.png" title="Untitled_2.png"><img style="width:288px" src="Untitled_2.png"></a>
        <figcaption>
          (Zdroj obrÃ¡zku: <a href="https://twitter.com/QasimMunye/status/1278750809094750211">https://twitter.com/QasimMunye/status/1278750809094750211</a>)
        </figcaption>
      </figure>
      <p id="0cc046b3-ad73-4794-98f6-bf0841c42858" class="">NormÃ¡lnÃ­m pÃ­smem je napsanÃ½ vstup pro GPT-3, tuÄnÄ› je napsanÃ¡ jeho odpovÄ›Ä. Nejen Å¾e sprÃ¡vnÄ› pochopil o Äem otÃ¡zka je, ale navÃ­c na zÃ¡kladÄ› textovÃ©ho popisu nemoci korektnÄ› urÄil o jakou nemoc se jednÃ¡, jakÃ½ na nÃ­ pouÅ¾Ã­t lÃ©k a na jakÃ© receptory v mozku ten lÃ©k pÅ¯sobÃ­.</p>
      <p id="42961632-b718-4c35-bb51-a165522f91ad" class="">Zde se zaÄÃ­nÃ¡ ukazovat sÃ­la GPT-3; protoÅ¾e nenÃ­ specializovanÃ½ na nic, mÃ¡ ohromnÃ½ pÅ™ehled ÃºplnÄ› o vÅ¡em. VÄetnÄ› diagnÃ³z nemocÃ­, lÃ©kÅ¯ a molekulÃ¡rnÃ­ biologie.</p>
      <h2 id="7f0b13ee-d700-4939-b83a-4591b7ed0ece" class="">Konverze poÅ¾adavkÅ¯ na unixovÃ© pÅ™Ã­kazy</h2>
      <p id="2bfeefc8-bc4d-46c1-85ab-f4bc29389736" class=""><a href="https://twitter.com/harlandduman/status/1282132804034150400">Harland Duman zkusil</a> pouÅ¾Ã­vat GPT-3 ukÃ¡zkou kterou majÃ­ v pÃ­skoviÅ¡ti pro testovÃ¡nÃ­ api. Ta funguje tak, jÃ­ popÃ­Å¡ete co chce aby udÄ›lala v shellu operaÄnÃ­ho systÃ©mu (modÅ™e vybranÃ½ text je â€pÅ™edpÅ™ipravenÃ­â€œ). GPT-3 potÃ© vypisuje konkrÃ©tnÃ­ pÅ™Ã­kazy:</p>
      <div id="e25122a6-619d-40d9-a1f4-88920c5d8bf3" class="column-list">
        <div id="b144e9ab-81c4-4e9f-b39b-a0527ddfa745" style="width:50%" class="column">
          <figure id="d82fbc02-8150-42ba-80b0-778dc7a5b728" class="image">
            <a href="Untitled_3.png" title="Untitled_3.png"><img style="width:1486px" src="Untitled_3_thumb.jpg"></a>
          </figure>
        </div>
        <div id="619d5ae0-0c4d-464a-af82-0e7ac42b6ed2" style="width:50%" class="column">
          <figure id="48074c74-e0d1-4f37-9602-e75c6c8cec50" class="image">
            <a href="Untitled_4.png" title="Untitled_4.png"><img style="width:1486px" src="Untitled_4_thumb.jpg"></a>
          </figure>
        </div>
      </div>
      <p id="d9e4064d-881c-4503-8b10-4fa8968343d0" class="">SÃ¡m jsem si s touhle verzÃ­ zkouÅ¡el hrÃ¡t a musÃ­m Å™Ã­ct, Å¾e zvlÃ¡dne i docela sloÅ¾itÃ© ukÃ¡zky zahrnujÃ­cÃ­ kombinace pomocÃ­ <code>pipes</code>, <code>find</code> a <code>xargs</code>. NapÅ™Ã­klad mu nedÄ›lÃ¡ problÃ©m vÄ›c jako komprese disku poslanÃ¡ pÅ™es <code>ssh</code>. Za zmÃ­nku stojÃ­, Å¾e celÃ½ proces funguje i obrÃ¡cenÄ›, tedy umÃ­ pÅ™evÃ©st pÅ™Ã­kaz na textovÃ½ popis, kterÃ½ ho vysvÄ›tlÃ­:</p>
      <div id="90237eff-ec34-4a63-9c32-d5221ae5e4d6" class="column-list">
        <div id="24017d9a-2219-4d11-bf17-6b121dbf9e43" style="width:50%" class="column">
          <figure id="a272794d-f426-43df-962f-16a3ebc3293f" class="image">
            <a href="Untitled_5.png" title="Untitled_5.png"><img style="width:1764px" src="Untitled_5_thumb.jpg"></a>
          </figure>
        </div>
        <div id="1597c3ed-1c2b-416e-8a02-7dd691d3eec3" style="width:50%" class="column">
          <figure id="b29d2112-0988-4488-abe9-57153b483ccf" class="image">
            <a href="Untitled_6.png" title="Untitled_6.png"><img style="width:1764px" src="Untitled_6_thumb.jpg"></a>
          </figure>
        </div>
      </div>
      <p id="5e3cf883-2db8-44fd-9a9a-b885e721f557" class="">Thread zahrnuje rÅ¯znÃ© i rÅ¯znÃ© dalÅ¡Ã­ ukÃ¡zky, napÅ™Ã­klad programovÃ¡nÃ­ v <code>node.js</code>, a taky zkouÅ¡ky toho jak moc GPT-3 chÃ¡pe ÄÃ­sla a rÅ¯znÃ© vztahy mezi nimi.</p>
      <h2 id="6f343064-8857-4d80-af43-28f3bfa3ef4f" class="">Design mobilnÃ­ch aplikacÃ­</h2>
      <p id="c286dc56-c874-40fe-8f91-54ce3e3cf9da" class=""><a href="https://twitter.com/jsngr/status/1284511080715362304">Jordan Singer</a> pouÅ¾il GPT-3 jako backend pro pÅ™evod textovÃ©ho popisu na layout mobilnÃ­ aplikace:</p>
      <div id="53147baf-a2de-4cd3-a337-d7674a30da04" class="column-list">
        <div id="55d572b2-f61b-4c41-b374-be2e785e1267" style="width:50%" class="column">
          <figure id="3b0e75ef-86d5-402e-9cb1-01a711f168f4" class="image">
            <a href="mobile_app_description.png" title="mobile_app_description.png"><img style="width:1008px" src="mobile_app_description_thumb.jpg"></a>
          </figure>
        </div>
        <div id="ab07a616-3077-4d41-9af4-4b515d4c8b45" style="width:50%" class="column">
          <figure id="61fbd5ec-7f4a-4b79-847b-cda21158dac4" class="image">
            <a href="mobile_app_design.png" title="mobile_app_design.png"><img style="width:1008px" src="mobile_app_design_thumb.jpg"></a>
          </figure>
        </div>
      </div>
      <h2 id="913969c9-b35a-4fba-8b68-72aaf511ccd3" class="">GenerovÃ¡nÃ­ kytarovÃ½ch not</h2>
      <p id="976bd107-227b-49a7-bb32-79252721eecf" class=""><a href="https://twitter.com/AmandaAskell/status/1283900372281511937">Amanda Askell vyzvala</a> GPT-3 aby generoval kytarovÃ© noty popsanÃ© v ASCII obrÃ¡zcÃ­ch:</p>
      <figure id="379bb8f1-814b-4d72-b140-bd8e7d4a860c" class="image">
        <a href="Untitled_7.png" title="Untitled_7.png"><img style="width:384px" src="Untitled_7.png"></a>
      </figure>
      <p id="b5739a71-2b77-4747-927b-f61e38877401" class="">Modelu staÄilo dÃ¡t dvÄ› ukÃ¡zky co od nÄ›j oÄekÃ¡vÃ¡. Nejen Å¾e vygeneroval vÃ½Å¡e uvedenÃ© obrÃ¡zky, ale i hudbu v nich, kterÃ¡ <a href="https://twitter.com/LoneVoltsAhead/status/1283920056582275072">neznÃ­</a> ÃºplnÄ› Å¡patnÄ›.</p>
      <p id="bf35b954-ac8b-4d84-bd32-212d98f68632" class="">Zde je pÄ›knÄ› vidÄ›t ukÃ¡zka toho, k Äemu se dÃ¡ model pouÅ¾Ã­t; tedy k transformaci dat z jednoho popisu do druhÃ©ho. NapÅ™Ã­klad generovÃ¡nÃ­ ASCII obrÃ¡zkÅ¯ podobnÃ©ho druhu by bylo bez nÄ›jakÃ©ho programu velmi otravnÃ©. Model vÅ¡ak zvlÃ¡dne tenhle pÅ™eklad pomÄ›rnÄ› jednoduÅ¡e jen na zÃ¡kladÄ› pÃ¡r ukÃ¡zek <em>vstup / vÃ½stup</em>.</p>
      <h2 id="c064a5f4-b98c-4558-9e68-faac6e2cee20" class="">A spousta dalÅ¡Ã­ho</h2>
      <h3 id="9373975c-2941-438c-8073-7f317350b99e" class="">SÃ©mantickÃ½ search engine</h3>
      <p id="db8a0ec9-f1e0-44f3-809d-b728c7344f32" class=""><a href="https://twitter.com/paraschopra/status/1284801028676653060">Paras Chopra</a> nad GPT-3 postavil sÃ©mantickÃ½ vyhledÃ¡vacÃ­ nÃ¡stroj, kterÃ©mu popÃ­Å¡ete co vÃ¡s zajÃ­mÃ¡ a on vÃ¡m to najde a vrÃ¡tÃ­ URL kde je detailnÄ›jÅ¡Ã­ popis. NÄ›co jako google, kterÃ½ ale rozumÃ­ vaÅ¡Ã­ otÃ¡zce, mÃ­sto aby vyhledÃ¡val podle klÃ­ÄovÃ½ch slov v textu.</p>
      <h3 id="df5ceb07-373f-44b7-a101-ba84983ec616" class="">UÄetnictvÃ­</h3>
      <p id="d0ab757c-44e0-458b-acc3-36fa7c98c87d" class="">UÅ¾ivatel s pÅ™ezdÃ­vkou <a href="https://twitter.com/itsyashdani/status/1285695850300219392">yash</a> vytvoÅ™il uÄetnictvÃ­, kterÃ©mu textovÄ› popisujete co jste udÄ›lali za transakci a ono to pÅ™evÃ¡dÃ­ do Å™eÄi ÄÃ­sel a faktur.</p>
      <h3 id="72203e36-1775-4935-be6d-71b7cb6a9698" class="">PÅ™eklad do prÃ¡vniÄiny</h3>
      <p id="eb7e9ab9-f07e-4b45-8d0e-c16da51e7d48" class=""><a href="https://twitter.com/f_j_j_/status/1283349995144359937">Francis Jervis</a> vyzval model aby pÅ™eloÅ¾il normÃ¡lnÄ› zadanÃ½ text do <em>â€prÃ¡vniÄinyâ€œ</em>, tedy jazyka pouÅ¾Ã­vanÃ©ho prÃ¡vnÃ­ky. VÃ½sledky jsou docela zajÃ­mavÃ©.</p>
      <h3 id="2c579351-bf3c-4e64-8519-f3a37fe46687" class="">VysvÄ›tlovÃ¡nÃ­, sumarizace a generovÃ¡nÃ­ textÅ¯</h3>
      <p id="20952e42-89ae-4290-a422-2f4ef46d7693" class=""><a href="https://twitter.com/Plinz/status/1283211048145711104">ZÃ¡bavnÃ¡ byla reakce Joschy Bacha</a>, kdyÅ¾ nÄ›kdo vzal jeho tweet o GPT-3, kterÃ½ ÃºplnÄ› nepochopil, a poÅ¾Ã¡dal GPT-3, aby ho vysvÄ›tlil.</p>
      <figure id="341c7555-c345-4c81-9110-f1d4e2a54583" class="image">
        <a href="Untitled_8.png" title="Untitled_8.png"><img style="width:1023px" src="Untitled_8_thumb.jpg"></a>
      </figure>
      <p id="a6c5c5fc-d851-4c07-bbbd-5f87e2690166" class="">Co se vysvÄ›tlovÃ¡nÃ­ textu tÃ½Äe, tak zajÃ­mavou mÃ­ru pochopenÃ­ a modelovÃ¡nÃ­ svÄ›ta ukazuje model v <a href="https://twitter.com/danielbigham/status/1292229584025464835">tomto tweetu</a>, kde byl dotazovÃ¡n na to ÄÃ­m jsou si vÄ›ci podobnÃ©.:</p>
      <figure id="8ecf0349-e6c8-4ec9-9fbe-72f635bd820a" class="image">
        <a href="Untitled_9.png" title="Untitled_9.png"><img style="width:432px" src="Untitled_9.png"></a>
      </figure>
      <h3 id="8880550e-9a08-4e6c-b66e-6d605e1f6635" class="">Tolik tedy k ukÃ¡zkÃ¡m</h3>
      <p id="bdfe26d5-e7fa-4c58-b7cc-f513586939ac" class="">Asi je z tÄ›ch ukÃ¡zek jasnÃ©, k Äemu to je. ObecnÄ› mÃ¡ model urÄitÃ© pochopenÃ­ textu, kterÃ© se nauÄil na pÅ™eÄtenÃ½ch datech. K tomu ale mÃ¡ taky â€znalostiâ€œ z tÄ›chto pÅ™eÄtenÃ½ch dat.</p>
      <p id="4936ecd5-b18a-4f89-8909-bbf347b13a4d" class="">Model funguje formou otÃ¡zka/odpovÄ›Ä a je moÅ¾nÃ© se ho tedy ptÃ¡t na rÅ¯znÃ¡ fakta, nechat ho odvozovat logickÃ© vÄ›ci, zvlÃ¡dÃ¡ ÄÃ¡steÄnÄ› i matematiku a symbolickÃ© uvaÅ¾ovÃ¡nÃ­ (umÃ­ napÅ™Ã­klad Å™eÅ¡it jednoduchÃ© rovnice). ObecnÄ› se dÃ¡ Å™Ã­ct, Å¾e vynikÃ¡ v pÅ™ekladu <em>pÅ™irozenÃ©ho jazyka</em> na nÄ›co jinÃ©ho. Na Å™eÅ¡enÃ­ otÃ¡zky. Na odpovÄ›Ä. Na kÃ³d. Mezi lidskÃ½mi jazyky.</p>
      <p id="793667f1-f129-497e-8d68-28dacd47483d" class="">Model rozhodnÄ› nefunguje bezchybnÄ›, ale i tak je to mnohem dÃ¡l, neÅ¾ vÅ¡echny pÅ™edchozÃ­ projekty. Kdybych to mÄ›l k nÄ›Äemu pÅ™irovnat, tak je zhruba pod ÃºrovnÃ­ velmi hloupÃ©ho ÄlovÄ›ka, kterÃ½ mÃ¡ ale ohromnÃ© (encyklopedickÃ©) znalosti na vÅ¡echna moÅ¾nÃ¡ tÃ©mata.</p>
      <p id="3d180783-8798-49d2-81f5-81bcf4fd0536" class="">To vÅ¡e bez specializace, tedy obecnÃ½ model. Tento model je moÅ¾nÃ© teoreticky dÃ¡le vzÃ­t a specificky ho dotrÃ©novat pomocÃ­ ukÃ¡zek, napÅ™Ã­klad z oboru <em>prÃ¡vniÄiny</em>, nebo v pÅ™ekladech, Äi k programovÃ¡nÃ­. TÃ­m se ÃºspÄ›Å¡nost jeÅ¡tÄ› zvÃ½Å¡Ã­. Nutno ovÅ¡em dodat, Å¾e tohle trÃ©novÃ¡nÃ­ mÅ¯Å¾e dÄ›lat jen nÄ›kdo kdo model vlastnÃ­, a nejednÃ¡ se o takzvanÃ© â€pÅ™edpÅ™ipravenÃ­â€œ zmÃ­nÄ›nÃ© vÃ½Å¡e, kterÃ© mÅ¯Å¾e udÄ›lat kdokoliv pouhou interakcÃ­ s modelem. VlastnÃ­kem je momentÃ¡lnÄ› pouze OpenAI.</p>
      <p id="36462c13-d1b3-4d8c-91f4-2b89863f2c3e" class="">PojÄme se nynÃ­ podÃ­vat jak to pÅ™ibliÅ¾nÄ› funguje.</p>
      <h1 id="ef0c9b3d-cd1d-471a-9905-08a500a23eb6" class="">TechnickÃ© detaily GPT-3</h1>
      <p id="f8ff231c-fd49-4990-b189-cb9dc8057fbf" class="">V pÅ™Ã­padÄ› GPT-3 jde o takzvanÃ½ <em>â€unsupervised learningâ€œ</em>, tedy druh strojovÃ©ho uÄenÃ­, kterÃ© se uÄÃ­ samo z dat. Princip je zhruba takovÃ½, Å¾e neuronovou sÃ­Å¥ krmÃ­me velkÃ½mi mnoÅ¾stvÃ­mi textÅ¯, a ona si v tom sama najde vzory.</p>
      <p id="2b543a01-fbc6-492b-a4f9-e2bc889fd387" class="">GPT-3 pracuje nad vektory <em>tokenÅ¯</em>, kterÃ© si mÅ¯Å¾eme pÅ™edstavit podobnÄ› jako v znÃ¡mÃ©m <a href="http://www.mlguru.com/cs/word2vec-jednoducha-aritmetika-se-slovy/">word2vec</a>.</p>
      <h2 id="7ff79ca7-8ef9-40e2-b9ad-ddb5abf9b611" class="">word2vec</h2>
      <p id="911ca32d-279c-4f94-9366-55a1a1ab2422" class="">Word2vec prorazil dÃ­ru do svÄ›ta pÅ™ed nÄ›kolika lety, kdyÅ¾ TomÃ¡Å¡ Mikolov publikoval v BrnÄ› svojÃ­ <a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf">dizertaÄnÃ­ prÃ¡ci</a> o pouÅ¾itÃ­ neuronovÃ© sÃ­tÄ›. JÃ­m popsanÃ¡ sÃ­Å¥ je schopnÃ¡ se na zÃ¡kladÄ› velkÃ©ho mnoÅ¾stvÃ­ textu sama nauÄit <a href="https://cs.wikipedia.org/wiki/Vno%C5%99en%C3%AD_slov">reprezentovat slova ve vÃ­cedimenzionÃ¡lnÃ­m prostoru</a> tak, Å¾e <strong>vÃ½znamovÄ› podobnÃ¡ slova</strong> tvoÅ™Ã­ v tomto prostoru clustery. ZÃ¡roveÅˆ jsou clustery v prostoru umÃ­stÄ›ny tak, Å¾e je moÅ¾nÃ© nad jejich reprezentacÃ­ provÃ¡dÄ›t vÃ½znamovou aritmetiku.</p>
      <p id="c4468501-506e-453f-b354-818a1c6eb69d" class="">Co si pod tÃ­m konkrÃ©tnÄ› pÅ™edstavit;</p>
      <p id="fcdf17b2-b25c-45e8-9611-0a037ec67470" class="">VÃ­ce-dimenzionÃ¡lnÃ­ prostor si mÅ¯Å¾ete pÅ™edstavit graficky napÅ™Ã­klad tak, Å¾e ke klasickÃ½m osÃ¡m <code>X</code>, <code>Y</code> a <code>Z</code> pÅ™idÃ¡te dalÅ¡Ã­. ZatÃ­mco bod ve 3D prostoru je urÄen maticÃ­ obsahujÃ­cÃ­ napÅ™Ã­klad souÅ™adnice ve tvaru <code>[1, 3, -10]</code>, bod v mnohodimenzionÃ¡lnÃ­m prostoru, kterÃ½ umÃ­ vytvÃ¡Å™et word2vec je tvoÅ™en cca sto aÅ¾ tisÃ­ci ÄÃ­sly popisujÃ­cÃ­mi jeho souÅ™adnice.</p>
      <p id="2393ea1b-1a22-48db-97ab-9945f95ce43d" class="">Clustery je moÅ¾nÃ© si pÅ™edstavit tak, Å¾e body slov v tomto prostoru, kterÃ© jsou vÃ½znamovÄ› podobnÃ©, jsou poblÃ­Å¾ sebe. Zde jsem si dovolil trochu upravit klasickÃ½ ukÃ¡zkovÃ½ obrÃ¡zek, aby byly dobÅ™e vidÄ›t dva rÅ¯znÃ© clustery, kterÃ© jsou v nÄ›m zakrouÅ¾kovÃ¡ny ÄervenÄ›. V jednom se nÃ¡m shlukujÃ­ muÅ¾skÃ© vÃ½razy, v druhÃ©m Å¾enskÃ©.</p>
      <figure id="a0729953-2a1d-4a2c-80c1-90102a76c7a1" class="image">
        <a href="clusters.png" title="clusters.png"><img style="width:384px" src="clusters.png"></a>
      </figure>
      <p id="2e44c243-ef3a-437d-9a45-044b8f582cbe" class="">Znovu opakuji, Å¾e pro jednoduchost a pÅ™edstavitelnost jsou pouÅ¾ity tÅ™i osy, protoÅ¾e zobrazit jich tam tisÃ­c nenÃ­ jednoduÅ¡e moÅ¾nÃ© ve dvouosÃ©m souÅ™adnicovÃ©m prostoru dostupnÃ©m pro obrÃ¡zky.</p>
      <p id="08c474e9-7e5c-47a8-833f-02e93404e101" class="">NynÃ­ se koneÄnÄ› dostÃ¡vÃ¡m, k tomu ÄÃ­m word2vec zaujal svÄ›t; k vÃ½znamovÃ© aritmetice. Nejen Å¾e totiÅ¾ umÃ­ vytvoÅ™it vÃ½Å¡e ukÃ¡zanÃ© clustery, ale zÃ¡roveÅˆ je v prostoru umÃ­sÅ¥uje tak, Å¾e mezi nimi jsou zachovÃ¡ny vztahy. MÅ¯Å¾ete tak napÅ™Ã­klad vzÃ­t vektor pro slovo <code>Å¾ena</code>, odeÄÃ­st od nÄ›j vektor pro slovo <code>muÅ¾</code>, a tento vÃ½sledek zachycujÃ­cÃ­ abstraktnÄ› pohlavÃ­ pÅ™iÄÃ­st k vektoru slova <code>krÃ¡l</code>, ÄÃ­mÅ¾ dostaneme vektor slova <code>krÃ¡lovna</code>.</p>
      <p id="c872a053-412d-4472-9aa5-6fba4142d2fe" class="">PÃ­Å¡u zÃ¡mÄ›rnÄ› <em>â€vektor slovaâ€œ</em>, vÃ½sledkem je totiÅ¾ matice ÄÃ­sel, kterÃ¡ urÄuje souÅ™adnice v mnohodimenzionÃ¡lnÃ­m prostoru. Tuto souÅ™adnici ovÅ¡em mÅ¯Å¾eme pÅ™eloÅ¾it zpÄ›t na text potÃ© co nad nÃ­ provedeme operace.</p>
      <p id="22a992ff-b900-4e68-af95-2b66c5243c27" class="">Zde jsou ukÃ¡zky rÅ¯znÃ½ch operacÃ­ nad vektory:</p>
      <figure id="5847cc4b-1183-48b5-a60e-e41efb46acbc" class="image">
        <a href="word2vec_examples.png" title="word2vec_examples.png"><img style="width:1818px" src="word2vec_examples_thumb.jpg"></a>
        <figcaption>
          <em>(Zdroj obrÃ¡zku:</em> <em><a href="https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html">Word Embeddings</a></em><em>)</em>
        </figcaption>
      </figure>
      <p id="bbb4e28a-cf4a-4f4a-b8d0-83a63014a538" class="">NapÅ™Ã­klad jde zjiÅ¡Å¥ovat Äasy jednotlivÃ½ch slov, nebo tÅ™eba hlavnÃ­ mÄ›sta zemÃ­.</p>
      <p id="702e46e8-7c0e-47dd-974a-3c9cb91601a1" class="">FascinujÃ­cÃ­ na tom je, Å¾e word2vec si sÃ¡m vytvÃ¡Å™Ã­ databÃ¡zi rÅ¯znÃ½ch faktÅ¯ a vztahÅ¯ mezi nimi, jen na zÃ¡kladÄ› toho, Å¾e ho nakrmÃ­me velkÃ½m mnoÅ¾stvÃ­m textu, ze kterÃ©ho si sÃ¡m tyto vztahy odvodÃ­.</p>
      <h2 id="7c7e6697-d4f5-480b-920c-0db29966b456" class="">ZpÄ›t k GPT-3</h2>
      <p id="eee94e4d-69a5-4781-bc2c-67b338e54323" class="">GPT-3 takÃ© operuje nad â€tokenyâ€œ, coÅ¾ jsou takÃ© mnohodimenzionÃ¡lnÃ­ souÅ™adnice ve vektorovÃ©m prostoru. MÅ¯Å¾e se jednat o samostatnÃ¡ slova, nebo nÄ›kdy <a href="https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_gpt2.py">mÅ¯Å¾e</a> dojÃ­t k rozdÄ›lenÃ­ na nÄ›kolik slabik, Äi podle unicode znakÅ¯. Detaily nejsou ÃºplnÄ› dÅ¯leÅ¾itÃ©. Na rozdÃ­l od word2vec je toto takzvanÃ© <em>â€embedovÃ¡nÃ­â€œ</em> slov jen pomÄ›rnÄ› nezajÃ­mavÃ½ vstupnÃ­ proces.</p>
      <p id="f2023c8b-c339-4080-9002-1558b6940ba0" class="">GPT-3 je algoritmus z rodiny <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">TransformerÅ¯</a>, tedy druhu architektury, jenÅ¾ se Äasto pouÅ¾Ã­vÃ¡ v NLP (<em>Natural Language Processing</em>, <em>zpracovÃ¡nÃ­ pÅ™irozenÃ© Å™eÄi</em>). Zkratka GPT znamenÃ¡ <em>Generative Pre-trained Transformer</em>, tedy <em>GenerativnÃ­ PÅ™edtrÃ©novanÃ½ Transformer</em>. Slovo â€generativnÃ­â€œ narÃ¡Å¾Ã­ na <a href="https://en.wikipedia.org/wiki/Generative_model">termÃ­n z machine learningu</a>.</p>
      <p id="8b2ae86e-0302-4ecc-8a53-7c8a3a3efca1" class="">Transformery pouÅ¾Ã­vajÃ­ takzvanÃ½ <a href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04">encoder-decoder model</a>. GPT a dalÅ¡Ã­ pouÅ¾Ã­vajÃ­ pouze <em>decodery</em>, kterÃ½ch na sebe pÅ™ipojÃ­ mnoho (24 v pÅ™Ã­padÄ› GPT-2, 96 v pÅ™Ã­padÄ› GPT-3). KaÅ¾dÃ½ <em>decoder</em> mÃ¡ vÃ­cero vrstev, z nichÅ¾ kaÅ¾dÃ¡ vstupnÃ­ vektory rÅ¯znÄ› hodnotÃ­, vytvÃ¡Å™Ã­ dalÅ¡Ã­ vektory definujÃ­cÃ­ vztahy s dalÅ¡Ã­mi slovy (vektory) ve vÄ›tÃ¡ch, pÅ™idÃ¡vÃ¡ indexy a obecnÄ› dalÅ¡Ã­ metadata, a tento vÃ½sledek pak posÃ­lÃ¡ dÃ¡l do neuronovÃ© sÃ­tÄ›. VelkÃ½ dÅ¯raz je kladen na <em>â€self-attentionâ€œ</em> vrstvu.</p>
      <p id="455ddfe9-b803-4e6b-a145-0030a920b152" class="">Zde je pro zajÃ­mavost vidÄ›t architektura <em>decoder</em> bloku z GPT-2:</p>
      <figure id="49991582-7681-4194-9f57-6a04ff1cb148" class="image">
        <a href="gpt2.png" title="gpt2.png"><img style="width:192px" src="gpt2.png"></a>
        <figcaption>
          (ObrÃ¡zek pochÃ¡zÃ­ z <a href="https://www.researchgate.net/figure/a-GPT-2-architecture-For-more-info-on-individual-operations-see-Vaswani-et-al-2017_fig1_335737829">https://www.researchgate.net/figure/a-GPT-2-architecture-For-more-info-on-individual-operations-see-Vaswani-et-al-2017_fig1_335737829</a>)
        </figcaption>
      </figure>
      <p id="7865bd59-f01b-4317-b2eb-5b671ebaf263" class="">Jay Alammar sepsal perfektnÃ­ sÃ©rii ÄlÃ¡nkÅ¯, kde vysvÄ›tluje trasformery a i GPT-2, na kterÃ©m je GPT-3 zaloÅ¾enÃ½, graficky a krÃ¡snÄ› do detailu:</p>
      <ul id="70eb2981-9c9b-49f8-80c1-75bc88196fa4" class="bulleted-list">
        <li>
          <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a>
        </li>
      </ul>
      <ul id="5bfe54be-9a1b-4f9d-944c-e839e4a69127" class="bulleted-list">
        <li>
          <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
        </li>
      </ul>
      <ul id="d302a080-2ba6-4529-b947-61d3e410270f" class="bulleted-list">
        <li>
          <a href="http://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a>
        </li>
      </ul>
      <p id="0f8bde30-daab-402c-82f0-b8e310ca7d55" class="">Specificky poslednÃ­ ÄlÃ¡nek vysvÄ›tluje <em>â€self-attentionâ€œ</em> vrstvy, tedy jak model vypoÄÃ­tÃ¡vÃ¡ kterÃ© slovo mÃ¡ jakou pozornost a souvislost s ostatnÃ­mi, a teprve pak se tÃ­m krmÃ­ vnitÅ™nÃ­ neuronovÃ© sÃ­tÄ›.</p>
      <p id="31ea6e65-5a3a-4f73-8a53-92a2de843254" class="">OficiÃ¡lnÃ­ paper uvÃ¡dÃ­ Å¾e k trÃ©novÃ¡nÃ­ sÃ­tÄ› bylo pouÅ¾ito cca 499 miliard tokenÅ¯, zahrnujÃ­cÃ­ch mimo jinÃ© sbÄ›r dat z internetovÃ½ch strÃ¡nek, ÄÃ¡sti wikipedie a rÅ¯znÃ© knihy. NapÅ™Ã­klad jen dataset <em>Common Crawl</em>, kterÃ½ tvoÅ™il pÅ™ibliÅ¾nÄ› 82% trÃ©novacÃ­ch dat, zabÃ­ral po vyfiltrovÃ¡nÃ­ a vyÄiÅ¡tÄ›nÃ­ 570GB v ÄistÃ© textovÃ© podobÄ›.</p>
      <p id="1e45ca1e-b9f2-4fd3-91aa-8db3875719fe" class="">TrÃ©novÃ¡nÃ­m na superpoÄÃ­taÄi byl stvoÅ™en model, kterÃ½ obsahuje 175 miliard parametrÅ¯. Parametry jsou jednak rÅ¯znÃ© vektory, urÄujÃ­cÃ­ napÅ™Ã­klad <em>self-attention</em> vrstvy, ale takÃ© pÅ™Ã­mo nastavenÃ­ neuronovÃ½ch sÃ­tÃ­. Model objemem parametrÅ¯ pÅ™ibliÅ¾nÄ› stokrÃ¡t pÅ™ekonÃ¡vÃ¡ pÅ™edchozÃ­ GPT-2.</p>
      <p id="83050b5b-09a0-45ed-81a2-9f90b4074378" class="">Pro ilustraci; je odhadovÃ¡no, Å¾e vÃ½kon potÅ™ebnÃ½ k trÃ©novÃ¡nÃ­ sÃ­tÄ› odpovÃ­dal zhruba 355 GPU let (autoÅ™i v paperu uvÃ¡dÄ›jÃ­ nÄ›kolik tisÃ­c petaflop/dnÅ¯), tedy let bÄ›hu modernÃ­ vÃ½konnÃ© grafickÃ© karty, coÅ¾ ÃºdajnÄ› <a href="https://lambdalabs.com/blog/demystifying-gpt-3/#1">odpovÃ­dÃ¡</a> ÄÃ¡stce ~4.6 milionÅ¯ dolarÅ¯.</p>
      <h2 id="1f0eb141-e8f6-41e3-b1e9-9c1cb58cc30d" class="">Jak funguje trÃ©novÃ¡nÃ­</h2>
      <p id="349eff3f-cb58-4aa6-b6aa-00b0e97e3b58" class="">VysokoÃºrovÅˆovÃ½ princip fungovÃ¡nÃ­ GPT-3 je krÃ¡snÄ› vysvÄ›tlen v ÄlÃ¡nku <a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">How GPT3 Works - Visualizations and Animations</a>. ProtoÅ¾e zde nehodlÃ¡m krÃ¡st animace, kterÃ© autor pouÅ¾il v ÄlÃ¡nku, zde je jen krÃ¡tkÃ½ textovÃ½ popis:</p>
      <p id="4d344479-8c9e-48b7-bdbe-c41ab0787028" class="">Dataset ukÃ¡zek textu je pouÅ¾it k trÃ©novÃ¡nÃ­ sÃ­tÄ› tak, aby predikovala vÃ½skyt nÃ¡sledujÃ­cÃ­ho tokenu. PodobnÄ› jako napÅ™Ã­klad markovovy Å™etÄ›zce umÃ­ na zÃ¡kladÄ› statistiky predikovat pravdÄ›podobnost vÃ½skytu dalÅ¡Ã­ho pÃ­smena, tak GPT modely na zÃ¡kladÄ› vstupnÃ­ch dat zkouÅ¡Ã­ odhadnout pravdÄ›podobnost vÃ½skytu dalÅ¡Ã­ho tokenu.</p>
      <p id="05b32dd4-e6de-40f9-9b3d-e35541518bc8" class="">Pokud se netrefÃ­ na oÄekÃ¡vanÃ½ token (prostÄ› dalÅ¡Ã­ slovo z datasetu), jsou pouÅ¾ity techniky trÃ©novÃ¡nÃ­ tak dlouho, dokud token nenÃ­ odhadnut sprÃ¡vnÄ›.</p>
      <p id="0f010dba-e5da-4aef-9231-fd05335d9687" class="">NapÅ™Ã­klad zadÃ¡me modelu na vstup vÄ›tu:</p>
      <blockquote id="3b073f25-1d1a-4d2d-bd02-489f84ac76f2" class="">
        Byl pozdnÃ­ veÄer â€“ prvnÃ­ mÃ¡j â€“ veÄernÃ­ mÃ¡j â€“ byl
      </blockquote>
      <p id="94455bd0-e859-4284-95ba-ea6f4f5d5912" class="">a oÄekÃ¡vÃ¡me doplnÄ›nÃ­ dalÅ¡Ã­ho slova. Pokud sÃ­Å¥ doplnÃ­ slovo â€lÃ¡skyâ€œ, pokraÄujeme dÃ¡l. Pokud ne, provÃ¡dÃ­me trÃ©novÃ¡nÃ­ a Ãºpravy rÅ¯znÃ½ch vektorÅ¯ vhodnÃ½mi algoritmy, dokud model sprÃ¡vnÄ› neodhadne slovo â€lÃ¡skyâ€œ. Jakmile se trefÃ­, pustÃ­me na vstup</p>
      <blockquote id="8f0edb46-cec5-46cd-9e89-e9968f57b898" class="">
        Byl pozdnÃ­ veÄer â€“ prvnÃ­ mÃ¡j â€“ veÄernÃ­ mÃ¡j â€“ byl lÃ¡sky
      </blockquote>
      <p id="36f0d140-b634-4137-970d-a6c64a9a3add" class="">.. a opakujeme postup, dokud netrefÃ­ sprÃ¡vnÄ› slovo â€Äasâ€œ.</p>
      <p id="fe16f672-641e-4680-8a23-ea2a8cebdf23" class="">Data, kterÃ¡ dÃ¡vÃ¡me modelu takto dokola na vstup nejsou nekoneÄnÃ¡, tvoÅ™Ã­ jakÃ©si posuvnÃ© â€kontextovÃ© okÃ©nkoâ€œ, kterÃ© mÃ¡ v pÅ™Ã­padÄ› GPT-3 dÃ©lku 2048 tokenÅ¯.</p>
      <p id="63f12a4e-3e12-4b3d-8187-5ae752462024" class="">Algoritmus pro trÃ©novÃ¡nÃ­ byl pouÅ¾it Adam (detaily nastavenÃ­ v paperu na stranÄ› 43), coÅ¾ je optimalizaÄnÃ­ technika zaloÅ¾enÃ¡ na stochastickÃ©m sestupu gradientu, popsanÃ¡ napÅ™Ã­klad zde: <a href="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">Adam â€” latest trends in deep learning optimization</a>.</p>
      <h3 id="0ffefcef-5f67-49bd-b01f-30bbf8cc3076" class="">StochastickÃ½ sestup gradientu</h3>
      <p id="c5ea5a63-13a2-404a-96fc-29b246f36696" class="">Pod vÃ½razem <em>â€stochastickÃ½ sestup gradientuâ€œ</em> si pÅ™edstavte druh Ãºloh z matematiky, kterÃ© se zabÃ½vajÃ­ nalezenÃ­m globÃ¡lnÃ­ho minima (Äi maxima) s co nejmenÅ¡Ã­ nÃ¡mahou.</p>
      <p id="47b91e90-6f94-4b63-b87e-fa9ef16586f8" class="">NapÅ™Ã­klad pokud by se jednalo o 3D prostor, mÅ¯Å¾eme si pÅ™edstavit pohoÅ™Ã­ se spoustou ÃºdolÃ­ a kopcÅ¯, kterÃ© jsou tvoÅ™eny body v naÅ¡em prostoru, a cÃ­lem je najÃ­t poslepu to <strong>nejhlubÅ¡Ã­</strong> ÃºdolÃ­. MÃ¡me ale k dispozici jen prst, kterÃ½m mÅ¯Å¾eme do mapy ze-shora bodat, a tÃ­m prstem cÃ­tÃ­me do jakÃ© vÃ½Å¡ky jsme narazili a jestli se terÃ©n svaÅ¾uje nahoru, nebo dolu.</p>
      <figure id="18f6f376-a66e-4f27-82c0-fc5951882811" class="image">
        <a href="Untitled_10.png" title="Untitled_10.png"><img style="width:432px" src="Untitled_10.png"></a>
        <figcaption>
          <em>(ObrÃ¡zek pochÃ¡zÃ­ z</em> <em><a href="https://www.quora.com/Does-Gradient-Descent-Algo-always-converge-to-the-global-minimum">Does Gradient Descent Algo always converge to the global minimum?</a></em><em>)</em>
        </figcaption>
      </figure>
      <p id="e02ca645-a638-46e6-8499-0ae87220dcf1" class="">JednÃ­m z moÅ¾nÃ½ch algoritmÅ¯ je prostÄ› bodnout nÄ›kam nÃ¡hodnÄ› do mapy a pak zkusit bodat do kruhu kolem, jestli jsme se tÅ™eba netrefili do kopce a nÄ›kde kolem nenÃ­ sestup do ÃºdolÃ­. A kdyÅ¾ jo, tak to celÃ© zopakujeme tÃ­m smÄ›rem, kde je ÃºdolÃ­. NÄ›kterÃ© jinÃ© techniky pouÅ¾Ã­vajÃ­ napÅ™Ã­klad postup tÃ­m smÄ›rem, kudy se cesta minule svaÅ¾ovala dolu.</p>
      <figure id="4e6b81e0-e2a5-42ba-a4dc-e4a32531fbdb" class="image">
        <a href="Untitled_11.png" title="Untitled_11.png"><img style="width:480px" src="Untitled_11.png"></a>
        <figcaption>
          <em>(ObrÃ¡zek pochÃ¡zÃ­ z ÄlÃ¡nku</em> <em><a href="https://snlpatel0012134.wixsite.com/thinking-machine/single-post/StochasticGradientDescent-SGD">What is Stochastic Gradient Descent (SGD)</a></em><em>)</em>
        </figcaption>
      </figure>
      <p id="aa3df4fc-aa00-4624-8235-2f31feab2902" class="">StochastickÃ© algoritmy jsou druh matematickÃ½ch algoritmÅ¯, kterÃ© se snaÅ¾Ã­ s co nejmenÅ¡Ã­m poÄtem bodnutÃ­ do mapy najÃ­t ne jen nÄ›jakÃ© ÃºdolÃ­, ale rovnou to <strong>nejhlubÅ¡Ã­</strong> ÃºdolÃ­. To mÅ¯Å¾e zahrnovat rÅ¯znÃ© chytristiky, jako tÅ™eba <em>â€nevzdÃ¡vej se potom co narazÃ­Å¡ na prvnÃ­ ÃºdolÃ­, ale zkus jeÅ¡tÄ› sejÃ­t z kopce jinÃ½m smÄ›remâ€œ</em>, nebo tÅ™eba opÄ›tovnÃ© lezenÃ­ na kopec, Äi nÃ¡hodnou zmÄ›na mÃ­sta do kterÃ©ho teÄ bodÃ¡me.</p>
      <p id="ca704214-2ab2-42f8-a9a8-f5c0425bc945" class="">Tahle Ãºloha nÃ¡m mÅ¯Å¾e pÅ™ipadat lehkÃ¡, ale jen protoÅ¾e se dÃ­vÃ¡me na kopec oÄima, kterÃ© kaÅ¾dÃ½m pohledem pÅ™ijÃ­majÃ­ miliardy fotonÅ¯ odraÅ¾enÃ½ch od kopce. Pokud bysme vnÃ­mali svÄ›t kolem sebe jen jednÃ­m fotonem, taky by bylo v naÅ¡em zÃ¡jmu mÃ­t algoritmus, kterÃ½m si vnÃ­mÃ¡nÃ­ svÄ›ta kolem sebe co nejvÃ­c zrychlÃ­me. KaÅ¾dÃ© testovÃ¡nÃ­ kopce nÃ¡s stojÃ­ bodnutÃ­ prstem do mapy, vyslÃ¡nÃ­ Äi pÅ™ijmutÃ­ fotonu, nebo ÄistÄ› prakticky vÃ½poÄetnÃ­ instrukce, a tedy Äas a energii.</p>
      <p id="1fc18a09-4c86-4eec-8019-8d3fda53b018" class="">Adam je efektivnÃ­ druh hledÃ¡nÃ­ mnohodimenzionÃ¡lnÃ­ch ÃºdolÃ­ v mnohodimenzionÃ¡lnÃ­ch prostorech. ZpÅ¯sob jakÃ½m funguje je kompromis, aby bylo v datech tÅ™eba co nejmÃ©nÄ›-krÃ¡t bodnout do mapy (tedy zjistit jakÃ¡ je tam hodnota a sklon), a celÃ© to tedy fungovalo co nejrychleji a naÅ¡lo to zÃ¡roveÅˆ co nejhlubÅ¡Ã­ ÃºdolÃ­.</p>
      <p id="fec0aba4-d1a8-4e6e-bb91-8c559a535034" class=""><a href="https://www.abclinuxu.cz/lide/hrachj">Jenda z abclinuxu</a> pÄ›knÄ› popsal jak se to pak pouÅ¾Ã­vÃ¡:</p>
      <ul id="9ec7dee6-9504-4758-ae91-29641e09650e" class="bulleted-list">
        <li>Hodnota chybovÃ© funkce je poÄÃ­tanÃ¡ jako â€kolik procent chybÃ­ slovu â€lÃ¡skyâ€œ z pÅ™edchozÃ­ho pÅ™Ã­kladu do 100%â€œ, a toto posÄÃ­tanÃ© pÅ™es vÅ¡echny pÅ™Ã­klady z celÃ©ho trÃ©novacÃ­ho setu.</li>
      </ul>
      <ul id="a076aee0-1e58-4bbf-a3e3-fc844daea2ab" class="bulleted-list">
        <li>TerÃ©n je funkce R^175000000 â†’ R. Funkci se dajÃ­ aktuÃ¡lnÃ­ vÃ¡hy modelu a ona vrÃ¡tÃ­ hodnotu chyby. A my ji chceme minimalizovat.</li>
      </ul>
      <ul id="07b52140-ec33-4662-b4fb-285d13d311b1" class="bulleted-list">
        <li>PÅ™i minimalizaci ale nemÅ¯Å¾eme hrabat pÅ™Ã­mo na tuto funkci, jednak protoÅ¾e jejÃ­ vyhodnocenÃ­ je pÅ™Ã­Å¡ernÄ› drahÃ© (znamenÃ¡ to vyhodnotit a posÄÃ­tat nÄ›co pÅ™es vÅ¡echny prvky datasetu), jednak protoÅ¾e mÃ¡ lokÃ¡lnÃ­ minima ve kterÃ½ch bychom se zasekli.</li>
      </ul>
      <ul id="c6111ec4-e7d1-4b8a-977b-aaec2e103c84" class="bulleted-list">
        <li>Proto dÄ›lÃ¡me to, Å¾e vytÃ¡hneme z datasetu jenom pÃ¡r samplÅ¯ (tomu se Å™Ã­kÃ¡ minibatch), a chybu budeme vyhodnocovat na nich. TÃ­m zÃ­skÃ¡me nÄ›jakÃ½ trochu jinÃ½ terÃ©n, kterÃ½ snad bude podobnÃ½ tomu â€globÃ¡lnÃ­muâ€œ terÃ©nu, ale dÃ¡ se s nÃ­m poÄÃ­tat.</li>
      </ul>
      <ul id="e74f5485-b3a4-4e0c-90a8-47eee46bf354" class="bulleted-list">
        <li>NynÃ­ spoÄÃ­tÃ¡me aktuÃ¡lnÃ­ vÃ½Å¡ku tohoto terÃ©nu, urÄÃ­me, kterÃ½m smÄ›rem je to z kopce, a tÃ­mto smÄ›rem kousek popojdeme.</li>
      </ul>
      <ul id="1b4bc866-fc04-4412-bd90-1b25ce93845f" class="bulleted-list">
        <li>VytÃ¡hneme dalÅ¡Ã­ch pÃ¡r samplÅ¯ a opakujeme. Pro tyto jinÃ© samply bude terÃ©n vypadat troÅ¡ku jinak, a napÅ™Ã­klad doufÃ¡me, Å¾e lokÃ¡lnÃ­ minima budou v jinÃ½ch mÃ­stech, takÅ¾e pokud jsme do nÄ›jakÃ©ho vstoupili, tak teÄ se trochu posunulo a zase z nÄ›j vylezeme.</li>
      </ul>
      <p id="88c5bac5-430b-4b22-9241-3dededff66c3" class="">Tohle celÃ© dÄ›lÃ¡me na kopci, kterÃ½ mÃ¡ 175 miliard dimenzÃ­.</p>
      <h1 id="eab1d7c0-3608-4b79-8a2d-5ac7865d30e2" class="">MoÅ¾nosti pouÅ¾itÃ­</h1>
      <p id="1dcad963-5dc9-4455-86ad-edf32e99a391" class="">Å˜Ã­kÃ¡ se, Å¾e poslednÃ­ renesanÄnÃ­ ÄlovÄ›k byl pravdÄ›podobnÄ› <em>Leonardo da Vinci</em>. Tedy ÄlovÄ›k, kterÃ½ znal vÅ¡echna umÄ›nÃ­ a Å™emesla svÃ© doby. Od tÃ© doby, pravÃ­ rÄenÃ­, je svÄ›t tak sloÅ¾itÃ½ a specializovanÃ½, Å¾e nikdo nemÅ¯Å¾e znÃ¡t vÅ¡echno.</p>
      <p id="209b2a7b-3ec7-45bb-b27f-e0f862723afc" class="">GPT-3 sice nenÃ­ ÄlovÄ›k, inteligenÄnÄ› je o hodnÄ› nÃ­Å¾, ale renesanÄnÃ­ rozhodnÄ› je. TÃ­m Å¾e se uÄil ÄtenÃ­m textÅ¯ vÃ­ nÄ›co prakticky o vÅ¡em, o Äem nÄ›co Äetl. Jeho moÅ¾nosti chÃ¡pÃ¡nÃ­, udrÅ¾enÃ­ kontextu a logicky odvozovat vÄ›ci jsou omezenÃ©, ale jeho znalosti dost moÅ¾nÃ¡ uÅ¾ v souÄasnosti jsou vÄ›tÅ¡Ã­, neÅ¾ libovolnÃ©ho jinÃ©ho Å¾ijÃ­cÃ­ho ÄlovÄ›ka.</p>
      <p id="1d6c0310-6574-4d5f-837e-db6598d853d4" class="">ÄŒlÃ¡nek je moÅ¾nÃ¡ ladÄ›n v aÅ¾ moc pozitivnÃ­m duchu; je tÅ™eba pÅ™iznat si, Å¾e GPT-3 mÃ¡ stÃ¡le spoustu omezenÃ­, dÄ›lÃ¡ spoustu stupidnÃ­ch chyb a celkovÄ› rozhodnÄ› nenÃ­ dokonalÃ½.</p>
      <p id="33baf435-a9c0-44ad-863a-fc061eb08dde" class="">Na druhou stranu je nutnÃ© dodat, Å¾e i souÄasnÃ¡ verze je snad jako prvnÃ­ <em>pouÅ¾itelnÃ¡</em> pro spoustu vÄ›cÃ­, i se vÅ¡emi svÃ½mi omezenÃ­mi. To se stÃ¡le bavÃ­me o verzi, kterÃ¡ nenÃ­ specificky trÃ©novanÃ¡ pro konkrÃ©tnÃ­ Äinnosti (takzvanÃ½ fine-tuning), coÅ¾ mÃ¡ bÃ½t dalÅ¡Ã­ feature kterou chce OpenAI zpÅ™Ã­stupnit.</p>
      <p id="89802708-5010-4f67-9de6-73cdeb7cca6a" class="">PojÄme se nynÃ­ podÃ­vat, jak se to vlastnÄ› celÃ© prakticky pouÅ¾Ã­vÃ¡.</p>
      <h2 id="31586649-c9a1-4ea6-b730-6d1cc133ddfd" class="">UkÃ¡zka API</h2>
      <p id="4ee69146-2b81-4a16-8b39-f415b83db1c2" class="">PÅ™Ã­stup do API se nachÃ¡zÃ­ na adrese <a href="https://beta.openai.com">https://beta.openai.com</a>. Zde je moÅ¾nÃ© najÃ­t nÄ›jakou zÃ¡kladnÃ­ dokumentaci (rozÅ¡Ã­Å™enÃ¡ deprecated dokumentace je <a href="https://www.notion.so/API-Developer-Toolkit-49595ed6ffcd413e93ebff10d7e70fe7">na notionu</a>), popis pouÅ¾itÃ­, tutoriÃ¡ly a rÅ¯znÃ© dalÅ¡Ã­ relevantnÃ­ informace.</p>
      <figure id="54c0e73c-558b-4b36-9aa8-9144146e3e31" class="image">
        <a href="welcome.png" title="welcome.png"><img style="width:1745px" src="welcome_thumb.jpg"></a>
      </figure>
      <p id="70b3842d-f4aa-445f-98cc-993cb0fd2b4b" class="">Vpravo nahoÅ™e je moÅ¾nÃ© si vybrat z nÄ›kolika rÅ¯znÃ½ch modelÅ¯, OpenAI samotnÃ© doporuÄuje model <code>davinci</code>. OstatnÃ­ modely jsou takÃ© pojmenovÃ¡ny podle rÅ¯znÃ½ch historickÃ½ch postav.</p>
      <figure id="8bcd2ff6-1144-412d-a3ad-13ff7cc6c9a4" class="image">
        <a href="abclinuxu_question.png" title="abclinuxu_question.png"><img style="width:1745px" src="abclinuxu_question_thumb.jpg"></a>
      </figure>
      <p id="df9ebebd-a6a8-4239-9b2c-f9571ef9d7c9" class="">KaÅ¾dÃ¡ ukÃ¡zka se takÃ© dÃ¡ zobrazit jako volÃ¡nÃ­ API pomocÃ­ CURL nebo Pythonu. Vpravo je moÅ¾nÃ© vybrat si rÅ¯znÃ© parametry, kterÃ© model konfigurujÃ­ co se tÃ½Äe dÃ©lky vrÃ¡cenÃ©ho textu, nÃ¡hodnosti a tak podobnÄ›.</p>
      <h2 id="ed12eefe-c9dd-4e4e-a241-477f7845b2bc" class="">PouÅ¾itelnost</h2>
      <p id="2082643e-aa2b-495c-aa26-5ca4498c467a" class="">Celkem nepÅ™ekvapivÄ› se vÄ›tÅ¡ina prÃ¡ce s GPT-3 smrskÃ¡vÃ¡ do vytvoÅ™enÃ­ vhodnÃ©ho â€pÅ™edpÅ™ipravenÃ­â€œ, a nastavenÃ­ sprÃ¡vnÃ½ch parametrÅ¯. Jak uÅ¾ jsem vysvÄ›tloval, model jen doplÅˆuje slova (tokeny). Pokud po nÄ›m nÄ›co chcete, je tÅ™eba ho dostat do â€nÃ¡ladyâ€œ tak, aby mohl doplnit oÄekÃ¡vanÃ© vÃ½sledky.</p>
      <p id="3967c04d-f2a4-47a2-9b6a-6dfe135cf0ce" class="">To bohuÅ¾el nemusÃ­ bÃ½t jednoduchÃ©. ÄŒasto se mi tÅ™eba podaÅ™ilo vyvolat divokÃ© chovÃ¡nÃ­, kdyÅ¾ jsem Å¡patnÄ› nastavil nÄ›jakÃ½ parametr. ObÄas model prostÄ› napÃ­Å¡e, Å¾e odmÃ­tÃ¡ odpovÄ›dÄ›t, a chovÃ¡ se tak trochu jako naÅ¡tvanÃ© dÃ­tÄ›. Jindy si prostÄ› vymyslÃ­ krycÃ­ historku, kterou se totÃ¡lnÄ› utrhne ze Å™etÄ›zu oÄekÃ¡vanÃ©ho vÃ½stupu, nebo se pustÃ­ do <a href="https://cs.wikipedia.org/wiki/Tautologie">tautologiÃ­</a>.</p>
      <p id="92ab65c5-f6ea-46ec-bd34-48124f6f89c5" class="">Co se tÃ½Äe generovÃ¡nÃ­ textu, pÅ™evodu na rÅ¯znÃ© jinÃ© popisy, nebo vysvÄ›tlovÃ¡nÃ­ vÄ›cÃ­, daÅ™ilo se mi bÄ›hem chvÃ­le dosahovat oÄekÃ¡vanÃ½ch vÃ½sledkÅ¯. U rÅ¯znÃ½ch jinÃ½ch ÃºkonÅ¯ jsem uÅ¾ ale zas tak moc ÃºspÄ›Å¡nÃ½ nebyl, a oÄekÃ¡vÃ¡m, Å¾e nejspÃ­Å¡ vyÅ¾adujÃ­ netriviÃ¡lnÃ­ mnoÅ¾stvÃ­ Äasu hranÃ­ si s parametry. NapÅ™Ã­klad rÅ¯znÃ© odvozovÃ¡nÃ­ faktÅ¯, Äi snaha donutit model generovat ASCII arty (jako napÅ™Ã­klad ty noty v ukÃ¡zkÃ¡ch nahoÅ™e), mi vÅ¯bec nevyÅ¡la podle oÄekÃ¡vÃ¡nÃ­.</p>
      <p id="72645e4a-f273-494f-89ff-5c4116387cd4" class="">MyslÃ­m Å¾e se zpÅ™Ã­stupnÄ›nÃ­m API se otevÅ™e novÃ¡ pozice â€kormidelnÃ­kaâ€œ vÃ½stupu, tedy druh specializace lidÃ­, jenÅ¾ budou nabÃ­zet generovÃ¡nÃ­ â€pÅ™edpÅ™ipravenÃ­â€œ a nastavenÃ­ parametrÅ¯ pro Å™eÅ¡enÃ­ konkrÃ©tnÃ­ch problÃ©mÅ¯.</p>
      <h1 id="252dc246-fd5d-4005-b067-d1c0efdeb9e0" class="">OtÃ¡zky</h1>
      <h2 id="e69d00bb-4d0c-4469-a0d7-df286c9a3313" class="">PÅ™ed-singularita</h2>
      <p id="09f29865-4183-4309-b586-586481c52bde" class="">ÄŒlovÄ›k se samozÅ™ejmÄ› musÃ­ zamyslet nad tÃ­m, kam tohle spÄ›je. Vybavuje se mi ilustrace z ÄlÃ¡nku <a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">Tima Urbana o umÄ›lÃ© inteligenci</a>:</p>
      <figure id="01a359dc-a4b5-4db2-b7c1-51e320ebde19" class="image">
        <a href="Untitled_12.png" title="Untitled_12.png"><img style="width:1376px" src="Untitled_12_thumb.jpg"></a>
        <figcaption>
          <em>(ObrÃ¡zek pochÃ¡zÃ­ z ÄlÃ¡nku</em> <em><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">The AI Revolution: The Road to Superintelligence</a></em><em>)</em>
        </figcaption>
      </figure>
      <p id="5fae6074-9517-4d80-933a-11fc0ce6b1cd" class="">GPT-2 byla taky takovÃ¡ roztomilÃ¡ opiÄka, kterÃ¡ umÄ›la doplÅˆovat texty. SÃ¡m jsem si s nÃ­ chvÃ­li hrÃ¡l a krmil jÃ­ kousky textÅ¯ mÃ½ch oblÃ­benÃ½ch autorÅ¯, naÄeÅ¾ mÄ› fascinovalo, kdyÅ¾ pokraÄovala pÅ™esnÄ› jejich stylem, i kdyÅ¾ z vÄ›tÅ¡iny se jednalo o text, kterÃ½ nedÃ¡val moc velkÃ½ smysl.</p>
      <p id="2c67691a-c781-4837-a409-dd01c9a6d56d" class="">Jeden den jsem si Å™Ã­kal, jak nÃ¡m to vÃ½voj na poli umÄ›lÃ© inteligence roztomile pokraÄuje, druhÃ½ den najednou ÄumÃ­m Å¾e GPT-3 nenÃ­ zas o tolik nÃ­Å¾, neÅ¾ ÃºroveÅˆ pro â€<em>Dumb human</em>â€œ, tedy hloupÃ©ho ÄlovÄ›ka.</p>
      <p id="c653541f-cfec-402c-b2aa-c19f24d0188a" class="">CoÅ¾ je zarÃ¡Å¾ejÃ­cÃ­, vzhledem k tomu Å¾e se jednÃ¡ o stejnÃ½ druh <em>machine learningu</em> jako GPT-2, jen natrÃ©novanÃ½ na vÄ›tÅ¡Ã­m mnoÅ¾stvÃ­ dat.</p>
      <p id="4f684bf2-7d1a-4794-9ceb-c4a1aad1cc4b" class="">Architektura GPT-3 je do jistÃ© mÃ­ry velmi hloupÃ¡. Kam to asi tak pÅ¯jde dotÃ¡hnout, pokud se bude trÃ©novat na specifickÃ© dovednosti, ale napÅ™Ã­klad se zvÄ›tÅ¡Ã­ kontextovÃ© okno, pÅ™idajÃ­ rÅ¯znÃ© druhy pamÄ›ti (krÃ¡tkodobÃ¡, dlouhodobÃ¡), matematickÃ© koprocesory a tak dÃ¡l? MomentÃ¡lnÄ› probÃ­hÃ¡ trÃ©ning pouze na textu sklizenÃ©m z internetu. Co kdyÅ¾ k tomu pÅ™ihodÃ­me napÅ™Ã­klad blok symbolickÃ© matematiky a donutÃ­me AI se s nÃ­m nauÄit pracovat?</p>
      <h2 id="7d9ba8f5-e126-4b73-9ccc-d2819ad64cb4" class="">Demokratizace AI</h2>
      <p id="429f708d-98af-417b-91a9-f3147a6b3d56" class="">PÅ™ed nÄ›kolika lety jsem Äetl <a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">ÄlÃ¡nky</a> Tima Urbana na tÃ©ma umÄ›lÃ© inteligence, nebo moÅ¾nÃ¡ lÃ©pe strojovÃ©ho uÄenÃ­, a snah <em>Elona Muska</em> demokratizovat ho. PÅ™estoÅ¾e jsem s nimi souhlasil, tak jsem je skuteÄnÄ› nechÃ¡pal, ne tak jak je chÃ¡pu teÄ, kdyÅ¾ jsem mÄ›l na vlastnÃ­ kosti moÅ¾nost zaÅ¾Ã­t si Å¡ok z pokroku.</p>
      <p id="489d80f9-74a4-4286-b936-9957d9161cb7" class="">PÅ™edstavte si model GPT-10, kterÃ½ je ve vÅ¡ech ohledech lepÅ¡Ã­ neÅ¾ ÄlovÄ›k. VÄ›tÅ¡Ã­ pochopenÃ­ psanÃ©ho textu, schopnost udrÅ¾et kontext, dÄ›lat matematiku, logiku, programovÃ¡nÃ­ a prostÄ› cokoliv. K tomu masivnÃ­ korpus znalostÃ­ celÃ©ho svÄ›ta.</p>
      <p id="99bdce62-1167-4193-8105-ed4b1fee0219" class="">I kdyÅ¾ pÅ™edpoklÃ¡dÃ¡me, Å¾e ho nebude ovlÃ¡dat Å¾Ã¡dnÃ¡ zlÃ¡ spoleÄnost, uÅ¾ samotnÃ© schopnosti, kterÃ© mÃ¡ GPT-3 jsou dost hustÃ© na to abych si dovedl pÅ™edstavit tisÃ­ce rÅ¯znÃ½ch uÅ¾iteÄnÃ½ch pouÅ¾itÃ­ k zÃ­skÃ¡nÃ­ vÃ½hody a nÃ¡skoku nad konkurencÃ­. HypotetickÃ© GPT-10 by spoleÄnosti, Äi vlÃ¡dÄ›, kterÃ¡ ho bude vlastnit, dÃ¡valo ohromnÃ© moÅ¾nosti.</p>
      <p id="d11c8eaf-787c-4508-974e-50eb9963591e" class="">Podle mÃ©ho je nutnÃ© vÃ­c podpoÅ™it demokratizaci AI, tedy ten proces, kdy autoÅ™i AI si ho nenechÃ¡vajÃ­ pro sebe, ale sdÃ­lejÃ­ ho se svÄ›tem, ale zÃ¡roveÅˆ taky vÅ¡ude moÅ¾nÄ› probÃ­hajÃ­ podobnÃ© experimenty.</p>
      <h2 id="ceb0493b-3b5b-4a24-a4ac-de362141ac57" class="">UzavÅ™enÃ½ pÅ™Ã­stup</h2>
      <p id="a8c1bbab-dc79-4034-948f-88299c935c96" class="">OpenAI se rozhodla, Å¾e model nezpÅ™Ã­stupnÃ­ veÅ™ejnosti ve formÄ› surovÃ½ch dat modelu, ale <a href="https://en.wikipedia.org/wiki/OpenAI#cite_note-gpt3-whynotfullmodel-65">plÃ¡nuje</a> ho Äasem zpÅ™Ã­stupnit formou placenÃ©ho pÅ™Ã­stupu k API. Tento pÅ™Ã­stup je momentÃ¡lnÄ› v beta reÅ¾imu. To znamenÃ¡, Å¾e mÅ¯Å¾ete poÅ¾Ã¡dat o pÅ™Ã­stup, ale zaÅ™adÃ­te se pouze k desÃ­tkÃ¡m tisÃ­c dalÅ¡Ã­ch ÄekajÃ­cÃ­ch, na kterÃ© se snad Äasem dostane. SÃ¡m jsem se k pÅ™Ã­stupu registroval pÅ™ed asi mÄ›sÃ­cem, a zatÃ­m se nikdo neozval.</p>
      <p id="ab2fce7b-dc07-4651-9af4-9d98d9767ab8" class="">AÄkoliv by se mi lÃ­bilo mÃ­t pÅ™Ã­stup pÅ™Ã­mo k modelu samotnÃ©mu, a mÃ­t tedy moÅ¾nost ho dÃ¡le trÃ©novat a dÄ›lat na nÄ›m experimenty, je nutnÃ© pÅ™ipomenout, Å¾e trÃ©novÃ¡nÃ­ a pravdÄ›podobnÄ› i bÄ›h modelu vyÅ¾aduje superpoÄÃ­taÄ. SuperpoÄÃ­taÄe jsou nejenom drahÃ© na poÅ™Ã­zenÃ­, ale takÃ© na provoz.</p>
      <p id="76fa78a4-96ec-431f-92bf-33f3fd22d993" class=""><a href="https://github.com/openai/gpt-3/issues/1">Issue na Githubu</a> projektu zmiÅˆuje nÄ›jakÃ¡ konkrÃ©tnÃ­ ÄÃ­sla, kterÃ¡ jsou ovÅ¡em zaloÅ¾enÃ¡ pouze na odhadech. ZmÃ­nÄ›no je 700+GB pamÄ›ti a cca 22 grafickÃ½ch karet, kaÅ¾dÃ¡ s 16GB RAM, s tÃ­m Å¾e i tak by model pravdÄ›podobnÄ› bÄ›Å¾el pomalu.</p>
      <p id="396d9b56-8656-4aaf-8d9b-a1b2694274e8" class="">Na nÄ›jakÃ½ vlastnÃ­ odhad nemÃ¡m dostateÄnÃ½ technickÃ½ backgroud. Faktem ovÅ¡em je, Å¾e OpenAI uzavÅ™ela partnerstvÃ­ s Microsoftem. Ten nedÃ¡vno <a href="https://blogs.microsoft.com/ai/openai-azure-supercomputer/">oznÃ¡mil</a>, Å¾e superpoÄÃ­taÄ pro OpenAI mÃ¡ bÃ½t provozovanÃ½ v Azure cloudu. Celkem by mÄ›l mÃ­t 285 000 CPU jader a 10 000 GPU.</p>
      <p id="2bf6cb9f-9bc6-44c0-9590-01176f65d0d6" class="">NepodaÅ™ilo se mi zjistit, jestli uÅ¾ byl pouÅ¾it pro trÃ©novÃ¡nÃ­ a provoz GPT-3, nebo se jednÃ¡ o budoucÃ­ projekt. NÄ›kterÃ¡ oznÃ¡menÃ­ jsou z roku 2019, a tvÃ¡Å™Ã­ se jako Å¾e byl nÄ›kdy koncem roku pÅ™edÃ¡n OpenAI, jinÃ¡ oznÃ¡menÃ­ se tvÃ¡Å™Ã­ jako Å¾e byl pÅ™edÃ¡n teprve nÄ›kdy v pÅ¯lce 2020.</p>
      <p id="39af327c-8c9e-4417-86ca-691513eb04ce" class="">KaÅ¾dopÃ¡dnÄ› to trochu dÃ¡vÃ¡ pÅ™edstavu ohlednÄ› hardware a ohlednÄ› ceny jednoho poÅ¾adavku na API.</p>
      <h3 id="e7b33810-df44-4bd3-94a8-3224a7b0d3cb" class="">Kde mÅ¯Å¾u zÃ­skat pÅ™Ã­stup</h3>
      <p id="590751f0-5151-4e14-ab46-f9ff5dac5d99" class="">OsobnÄ› mi byl pÅ™Ã­stup zapÅ¯jÄen nÄ›kÃ½m kdo ho uÅ¾ mÃ¡. Pokud ve svÃ©m sociÃ¡lnÃ­m okolÃ­ nikoho takovÃ©ho nemÃ¡te, je moÅ¾nÃ© zÃ­skat pÅ™Ã­stup pÅ™es hru <a href="https://play.aidungeon.io/">AI Dungeon</a>.</p>
      <p id="927af910-cd39-4c63-ae24-aec861cf90fe" class="">AI Dungeon vyuÅ¾Ã­vÃ¡ model GPT-2 pro hranÃ­ textovÃ© hry, kde si mÅ¯Å¾ete vybrat z nÄ›kolika rÅ¯znÃ½ch tÃ©matickÃ½ch svÄ›tÅ¯. <a href="https://play.aidungeon.io/subscribe">PlatÃ­cÃ­m hrÃ¡ÄÅ¯m</a> (tÃ½den zdarma, potom $9.99/mÄ›sÃ­c) vÅ¡ak <a href="https://medium.com/@aidungeon/ai-dungeon-dragon-model-upgrade-7e8ea579abfe">nabÃ­zÃ­</a> zprostÅ™edkovanÃ½ pÅ™Ã­stup k GPT-3. Ten je sice â€pÅ™edpÅ™ipravenÃ½â€œ scriptem pro uvedenÃ­ tÃ©ma textovÃ© hry, je moÅ¾nÃ© ho ovÅ¡em vlastnÃ­m textem â€pÅ™edpÅ™ipravitâ€œ na nÄ›co jinÃ©ho, a vyzkouÅ¡et si tak na nÄ›m interakce s GPT-3.</p>
      <p id="a2084e7a-8f1e-4119-9747-0f263224db7d" class="">Na twitteru se dajÃ­ najÃ­t pomÄ›rnÄ› zajÃ­mavÃ© ukÃ¡zky interakce s GPT-3. NÄ›kterÃ© jdou do pomÄ›rnÄ› metafyzickÃ½ch tÃ©mat, kdyÅ¾ napÅ™Ã­klad nÄ›kdo zjistil, Å¾e GPT-3 dÄ›lÃ¡ zÃ¡mÄ›rnÄ› v rozhovorech chyby, kterÃ© by dÄ›lala reprezentovanÃ¡ postava. Ukazuje se tedy Å¾e v â€hlavÄ›â€œ v rÃ¡mci autenticity modeluje osobnost postavy vÄetnÄ› chyb.</p>
      <figure id="10ae5755-4e24-4dbf-b7c7-33c5fa3db6e5" class="image">
        <a href="Untitled_13.png" title="Untitled_13.png"><img style="width:818px" src="Untitled_13.png"></a>
        <figcaption>
          <em>(Zdroj obrÃ¡zku:</em> <em><a href="https://twitter.com/kleptid/status/1284069270603866113">https://twitter.com/kleptid/status/1284069270603866113</a></em><em>)</em>
        </figcaption>
      </figure>
      <h2 id="279289fc-1b6d-4c5e-9a05-8af3e318e2f7" class="">GPT-4?</h2>
      <p id="4bc99410-0756-4d48-8529-2b45fdfd7a7b" class="">To Å¾e teÄ Äteme o GPT-3 znamenÃ¡, Å¾e jsme masivnÄ› pozadu, a Å¾e v OpenAI, ale pravdÄ›podobnÄ› i na mnoha dalÅ¡Ã­ch mÃ­stech na svÄ›tÄ›, se uÅ¾ vaÅ™Ã­ dalÅ¡Ã­ verze.</p>
      <p id="a534a16b-ea43-4b7a-83a6-76bae5e36355" class="">ZajÃ­mavÃ½ je model <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> (<a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">detaily</a>), kterÃ½ je vyvÃ­jenÃ½ v nÄ›kolika jinÃ½ch institucÃ­ch. Za zmÃ­nku taky stojÃ­ projekt <a href="https://huggingface.co/">HuggingFace</a>, kterÃ½ na githubu sjednocuje v jednom repozitÃ¡Å™i vÅ¡echny moÅ¾nÃ© architektury a datovÃ© zdroje pro mnoho rÅ¯znÃ½ch transformerÅ¯.</p>
      <p id="2ac529fa-5d2e-4c1f-8f13-a37d3e1838da" class="">Ten kdo mÃ¡ dnes pÅ™Ã­stup k superpoÄÃ­taÄi a moÅ¾nosti tam zkouÅ¡et a zkoumat alternativnÃ­ pÅ™Ã­stupy, ten mÅ¯Å¾e zÃ­tra doslova udÄ›lat dÃ­ru do svÄ›ta, jako se to povedlo tÅ™eba TomÃ¡Å¡ovi Mikolovi s word2vec.</p>
      <h2 id="695132c4-123a-4d40-adf5-03d5a2a82d80" class="">Hype</h2>
      <p id="9c8384c5-2441-4f8d-a912-374972e0666b" class="">AÄkoliv v ÄŒechÃ¡ch se zatÃ­m informace o GPT-3 prakticky nedostaly ani do odbornÃ© literatury, v anglicky hovoÅ™Ã­cÃ­ch mÃ©diÃ­ch vznikl â€hypeâ€œ, tedy jakÃ½si kult mÃ­sty aÅ¾ pÅ™ehnanÃ©ho adorovÃ¡nÃ­.</p>
      <p id="9d587857-7653-439f-b223-64927beedc87" class="">To samozÅ™ejmÄ› vedlo k proti-reakci, kde u spousty lidÃ­ je teÄ modernÃ­ GPT-3 odsuzovat, jako Å¾e je <em>â€pÅ™ehypovanÃ©â€œ</em> a k niÄemu.</p>
      <p id="7de90834-6842-4bde-93e3-33934fcbb7e2" class="">OsobnÄ› si myslÃ­m, Å¾e nemÃ¡ smysl podlÃ©hat ani jednÃ© vlnÄ›. GPT-3 je jen nÃ¡stroj, kterÃ½ mÅ¯Å¾eme zkusit pouÅ¾Ã­vat k nÄ›Äemu produktivnÃ­mu. NemÃ¡ smysl ho â€<em>hejtovatâ€œ</em> Å¾e je k niÄemu, kdyÅ¾ nezvlÃ¡dne vyÅ™eÅ¡it co po nÄ›m chceme, ani adorovat jako Å¾e vyÅ™eÅ¡Ã­ vÅ¡echny naÅ¡e problÃ©my.</p>
      <p id="e581b972-090a-456d-8b13-aaf85703fe4f" class="">OsobnÄ› se k tomu stavÃ­m asi jako ke kompilÃ¡toru; je uÅ¾iteÄnÃ© vÄ›dÄ›t, Å¾e to existuje, mÅ¯Å¾e bÃ½t uÅ¾iteÄnÃ© to zkusit pouÅ¾Ã­t na nÄ›jakÃ½ svÅ¯j projekt, kde mi uÅ¡etÅ™Ã­ prÃ¡ci. Teoreticky to mÃ¡ potenciÃ¡l pro automatizaci nudnÃ½ch opakovanÃ½ch zÃ¡leÅ¾itostÃ­.</p>
      <h1 id="2c2de3e0-ae2b-48b1-b1dc-68350c0f878c" class="">â€ZÃ¡vÄ›râ€œ</h1>
      <p id="8a144701-eedb-4dec-88cf-6370b28449d9" class="">Jako uÅ¾ jsme si Å™ekli, GPT-3 nenÃ­ nÃ¡stroj pro kaÅ¾dÃ©ho, a nenÃ­ to ani nÃ¡stroj pro kaÅ¾dou vÄ›c.</p>
      <p id="7e4c1911-0d64-4315-b5cb-6885259bb088" class="">OpenAI v roce 2018 pÅ™edvedlo prvnÃ­ generaci, kterÃ¡ dokÃ¡zala reagovat na text. UÅ¾ ta byla pomÄ›rnÄ› dobÅ™e pouÅ¾itelnÃ¡, ale pouze v rÃ¡mci nÄ›kolika tÃ©mat. A to je vlastnÄ› tak trochu celÃ½ problÃ©m.</p>
      <p id="d9d2721a-46fb-42e2-b6d8-870e8fde99f9" class="">DoufÃ¡m, Å¾e se Äasem podaÅ™Ã­ otevÅ™Ã­t zdroje, kterÃ© OpenAI vyuÅ¾Ã­vÃ¡, a bude moÅ¾nÃ© vyzkouÅ¡et trÃ©novÃ¡nÃ­ s rÅ¯znÃ½mi databÃ¡zemi a jinÃ½mi typy dat.</p>
      <p id="82a3af01-f46b-4d1f-9dd7-0e98df47bc11" class="">Pokud chceme GPT-3 hodnotit, je tÅ™eba si uvÄ›domit, Å¾e se jednÃ¡ o reÃ¡lnou vÄ›c, nikoliv o nÄ›jakou hypotetickou konstrukci nebo vÃ½zkumnÃ½ vÃ½sledek. V praxi je sice OpenAI GPT-3 jeÅ¡tÄ› daleko od cÃ­le, ale je to vlastnÄ› nejlepÅ¡Ã­ model Å¡iroce dostupnÃ½ veÅ™ejnosti.</p>
      <p id="83c2ca46-701d-4115-a1ef-4be0399d85c7" class="">A samotnÃ© GPT-3 pÅ™inÃ¡Å¡Ã­ obrovskÃ½ objem dat. To vÅ¡e je dÅ¯vodem, proÄ se dnes jednÃ¡ o jeden z nejvÃ½znamnÄ›jÅ¡Ã­ch vÃ½zkumnÃ½ch ÃºspÄ›chÅ¯ v oblasti AI.</p>
      <p id="b7f5d8c0-3e9c-4d47-a9cb-5c9729b352b7" class="">ZdÃ¡ se, Å¾e se vÅ¡echno docela rychle hÃ½be k nÄ›Äemu docela zajÃ­mavÃ©mu. Nejsem si jistÃ½, zda tohle je ono, ale velmi mÄ› to zaujalo. Prakticky kaÅ¾dÃ½ den se objevuje nÄ›jakÃ¡ novÃ¡ vÄ›c, kterÃ¡ se dÃ¡ pouÅ¾Ã­t pro vytvÃ¡Å™enÃ­ lepÅ¡Ã­ch algoritmÅ¯. JakÃ©koliv inovativnÃ­ nÃ¡pady, rÅ¯znÃ© experimenty, zajÃ­mavÃ© datovÃ© zdroje a koncepty jsou velmi vÃ­tÃ¡ny ;)</p>
      <h1 id="5639f029-ce4a-45ac-ac85-e3d3a1340dae" class="">SkuteÄnÃ½ zÃ¡vÄ›r</h1>
      <p id="da2b167c-aa10-4c1e-8c14-95fd793cd876" class="">PÅ¯vodnÄ› jsem vÃ½Å¡e uvedenÃ½ â€zÃ¡vÄ›râ€œ neplÃ¡noval, kdyÅ¾ jsem vÅ¡ak zkusil GPT-3 nakrmit ÄÃ¡stÃ­ tohoto ÄlÃ¡nku (bere jen 2048 tokenÅ¯), napsal ho sÃ¡m i s nÃ¡zvem kapitoly a markdownem pro nadpis. UmÃ­ totiÅ¾ i Äesky, i kdyÅ¾ pÃ¡r chyb (hlavnÄ› ve skloÅˆovÃ¡nÃ­) jsem musel opravit. SamozÅ™ejmÄ› takÃ© pÅ™edchozÃ­ ukÃ¡zku nezvlÃ¡dl na prvnÃ­ pokus, musel jsem to pustit pÅ™ibliÅ¾nÄ› desetkrÃ¡t, neÅ¾ jsem z nÄ›j dostal nÄ›co podobnÄ› koherentnÃ­ho. VÃ½sledek je sloÅ¾enÃ½ z nÄ›kolika prÅ¯bÄ›hÅ¯, ale i tak je zajÃ­mavÃ½.</p>
      <p id="4b2e8310-4bb8-4b0f-b2ee-f0c8033f8273" class="">Ukazuje to pÄ›knÄ› Äeho vÅ¡eho je model schopnÃ½. NapÅ™Ã­klad GPT-2 sice taky zvlÃ¡dal ÄeÅ¡tinu, ale velmi bÃ­dnÄ›, vÄ›ty pÅ¯sobily dost nekoherentnÃ­m dojmem a Äasto na ÄeÅ¡tinu odpovÃ­dal anglicky. UrÄitÄ› by nedokÃ¡zal takhle jednoduÅ¡e psÃ¡t sÃ¡m o sobÄ›.</p>
      <p id="7451a85d-52cd-4362-8e2f-65f8d9539aab" class="">Jsem zvÄ›davÃ½ kam tohle vÅ¡echno povede. MomentÃ¡lnÄ› je technologie <em>machine learningu</em> a â€<em>umÄ›lÃ© inteligence</em>â€œ stÃ¡le jeÅ¡tÄ› v plenkÃ¡ch, zÃ¡roveÅˆ se ale rozvÃ­jÃ­ dÄ›sivou rychlostÃ­.</p>
      <p id="38029e98-8e62-423e-a197-182abf3eec05" class="">UÅ¾ nÄ›kolik let si vedu poznÃ¡mky na tÃ©ma <em>â€k Äemu vÅ¡emu bych vyuÅ¾il osobnÃ­ho skÅ™etaâ€œ</em>, kam se snaÅ¾Ã­m shromaÅ¾Äovat druhy pracÃ­ a otravnostÃ­, u kterÃ½ch by bylo fajn, kdyby je za mÄ› Å™eÅ¡ila nÄ›jakÃ¡ automatizace. NapÅ™Ã­klad se jednÃ¡ o generovÃ¡nÃ­ metadat k rÅ¯znÃ½m projektÅ¯m.</p>
      <p id="d7fabeab-0813-43bb-890f-1b5e2d8d9930" class="">TÄ›Å¡Ã­m se na dobu, kdy budu moct pouÅ¾Ã­t nÄ›jakou podobnÄ› uÅ¾iteÄnou AI, jenÅ¾ by za mÄ› tyhle nudnÃ© tasky dÄ›lala, bez toho aniÅ¾ bych musel strÃ¡vit nÄ›kolik dnÃ­ nastavovÃ¡nÃ­m rÅ¯znÃ½ch Å¡ablonovacÃ­ch enginÅ¯ a psanÃ­m vlastnÃ­ch scriptÅ¯. Taky by se mi velmi hodilo nÄ›co, co dokÃ¡Å¾e prohledÃ¡vat poÄÃ­taÄ podle zadanÃ½ch pravidel a sÃ©manticky rozumÃ­ tomu co po tom chci. ObÄas se napÅ™Ã­klad snaÅ¾Ã­m najÃ­t email, Äi dokument, aniÅ¾ bych si pamatoval konkrÃ©tnÃ­ klÃ­ÄovÃ¡ slova.</p>
      <p id="024f371c-3f81-4f34-847a-bf64f9e85196" class="">Spousta lidÃ­ mÃ¡ ze stÃ¡le lepÅ¡Ã­ â€AIâ€œ (ve skuteÄnosti <em>machine learningu</em>) strach. Do jistÃ© mÃ­ry to chÃ¡pu. Je tÅ™eba si ovÅ¡em uvÄ›domit, Å¾e se nejednÃ¡ o magii, je to prostÄ› jen druh programu.</p>
      <p id="a1f07d70-d679-4259-ac25-f42a5ce90766" class="">Z hlediska â€<em>normÃ¡lnÃ­ch lidÃ­</em>â€œ mÅ¯Å¾ete bÃ½t v klidu; â€<em>umÄ›lÃ¡ inteligence</em>â€œ v tomhle podÃ¡nÃ­ je jednoduÅ¡Å¡Ã­ a dÃ¡vÃ¡ vÃ¡m vÄ›tÅ¡Ã­ moÅ¾nosti pouÅ¾itÃ­, neÅ¾ klasickÃ© poÄÃ­taÄe a programovÃ¡nÃ­. Pokud zvlÃ¡dnete delegovat prÃ¡ci a popsat zadÃ¡nÃ­ dalÅ¡Ã­mu ÄlovÄ›ku, zvlÃ¡dÃ¡te pouÅ¾Ã­vat i tenhle druh umÄ›lÃ© inteligence. NepÅ™ichÃ¡zÃ­te o moÅ¾nosti, naopak je tÃ­m zÃ­skÃ¡vÃ¡te.</p>
      <p id="fa5005e7-58c4-4c73-b3bf-a64f1b7afa5f" class="">TeÄ se jen postarat o to, aby mÄ›li vÅ¡ichni k dispozici pÅ™Ã­stup. Bylo by fajn, kdyby se z toho nestal zdroj Ãºtlaku drÅ¾enÃ½ v rukou nÄ›kolika jedincÅ¯ a korporacÃ­, kterÃ© ho vyuÅ¾ijÃ­ k vyÅ™azenÃ­ konkurence a horÅ¡Ã­m vÄ›cem (viz tÅ™eba ÄÃ­nskÃ¡ totalita a jejich sociÃ¡lnÃ­ kredit), ale nÃ¡stroj pro obohacenÃ­ moÅ¾nostÃ­ a schopnostÃ­ kaÅ¾dÃ©ho z nÃ¡s.</p>
      <h1 id="00db0416-e21d-4cda-8ec0-0c37bbe81cf1" class="">Edit <time>@2020/09/21</time></h1>
      <p id="3331eec2-405b-4228-adaf-928229d80a1e" class="">KrÃ¡snÃ½ talk s Joschou Bachem (mimochodem ÄlovÄ›k, kterÃ©ho doporuÄuji sledovat) na tÃ©ma GPT-3 a co dÄ›lÃ¡ a jak funguje:</p>
      <figure id="523bb265-68aa-4e18-bd27-08cdb388a1ae">
        <div class="source">
          <iframe width="100%" height="50%" frameborder="0" src="https://www.youtube.com/embed/FMfA6i60WDA" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </figure>
      <h1 id="c9a930ef-116a-4991-a2ec-aaaed0dfe3bc" class="">Zdroje</h1>
      <h2 id="839be2c4-5194-45ad-8fae-8ea18eecf81a" class="">ÄŒesky</h2>
      <ul id="117a6757-6297-4a5b-9d02-95243cfb40d8" class="bulleted-list">
        <li>
          <a href="https://cs.wikipedia.org/wiki/Vno%C5%99en%C3%AD_slov">VnoÅ™enÃ­ slov - wikipedie</a>
        </li>
      </ul>
      <ul id="6a07d55f-1684-4cfc-a240-2185c5f2edda" class="bulleted-list">
        <li>
          <a href="http://www.mlguru.com/cs/word2vec-jednoducha-aritmetika-se-slovy/">word2vec â€“ jednoduchÃ¡ aritmetika se slovy</a>
        </li>
      </ul>
      <ul id="5adb0a68-f4ea-4aed-9d0f-420c519551e3" class="bulleted-list">
        <li>
          <a href="https://investree.cz/umela-inteligence/">UmÄ›lÃ¡ inteligence Open AI, GPT-3 mÅ¯Å¾e bÃ½t nejvÄ›tÅ¡Ã­ inovacÃ­ od bitcoinu</a>
        </li>
      </ul>
      <ul id="fe41fb03-3743-49dd-a6f7-681e162c2bd3" class="bulleted-list">
        <li>
          <a href="https://www.zive.cz/clanky/microsoft-postavil-nejvykonnejsi-superpocitac-pro-umelou-inteligenci-bude-ho-vyuzivat-openai/sc-3-a-203979/default.aspx?fbclid=IwAR3HeV4PJnalChghkNyscXzMCSS6T7sPfgQ4jhXZ1twJbgWO3CfOvWsgIpk">Microsoft postavil nejvÃ½konnÄ›jÅ¡Ã­ superpoÄÃ­taÄ pro umÄ›lou inteligenci. Bude ho vyuÅ¾Ã­vat OpenAI</a>
        </li>
      </ul>
      <h2 id="4806df48-2c42-403f-bcac-40b8a879cf9a" class="">Anglicky</h2>
      <ul id="c38966f2-1c87-4e98-b56d-6d0ed0d0b6c5" class="bulleted-list">
        <li>
          <a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a>
        </li>
      </ul>
      <ul id="a5b09eb4-6191-491d-8eea-0d1bc15b90a1" class="bulleted-list">
        <li>
          <a href="https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html">Word Embeddings</a> (o word2vec a podobnÃ½ch)
        </li>
      </ul>
      <ul id="1db0a136-eaeb-455f-9df7-30f9e1bfc9e5" class="bulleted-list">
        <li>
          <a href="https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4">Word Embeddings for NLP</a>
        </li>
      </ul>
      <ul id="049f5169-7c99-463f-9b91-203e68a7f569" class="bulleted-list">
        <li>
          <a href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space">Embeddings: Translating to a Lower-Dimensional Space</a>
        </li>
      </ul>
      <ul id="03fda63d-a2e6-4b03-8f91-337e15c46967" class="bulleted-list">
        <li>
          <a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">How GPT3 Works - Visualizations and Animations</a>
        </li>
      </ul>
      <ul id="220fa9de-0b2c-4062-aad6-dee345649b4b" class="bulleted-list">
        <li>
          <a href="https://towardsdatascience.com/will-gpt-3-kill-coding-630e4518c04d">Will The Latest AI Kill Coding?</a>
        </li>
      </ul>
      <ul id="236d78c6-b7db-4fc3-a54a-2a53a3636de5" class="bulleted-list">
        <li>
          <a href="https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential">OpenAIâ€™s latest breakthrough is astonishingly powerful, but still fighting its flaws - The Verge</a>
        </li>
      </ul>
      <ul id="79fd665e-e7bd-4af5-8909-7e3a2e50506b" class="bulleted-list">
        <li>
          <a href="https://lambdalabs.com/blog/demystifying-gpt-3/">OpenAI's GPT-3 Language Model: A Technical Overview</a>
        </li>
      </ul>
      <ul id="2924d5ff-34d9-488a-8105-00963464a482" class="bulleted-list">
        <li>
          <a href="https://lambdalabs.com/blog/gpt-3/">GPT-3: A Hitchhiker's Guide</a>
        </li>
      </ul>
      <ul id="e5dd03bb-dce4-44e6-9357-dec3e17a8e91" class="bulleted-list">
        <li>
          <a href="https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/">Why GPT-3 Matters</a>
        </li>
      </ul>
      <ul id="2f959a2c-5e72-44ca-9ad0-2ab1ae15a751" class="bulleted-list">
        <li>
          <a href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04">What is a Transformer?</a>
        </li>
      </ul>
      <ul id="a1acede7-0278-45bd-aa31-0ca2d45e008a" class="bulleted-list">
        <li>
          <a href="https://github.com/openai/gpt-2/">https://github.com/openai/gpt-2/</a>
        </li>
      </ul>
      <ul id="532a9956-8630-4ca7-9ebc-8ae1b4d310d3" class="bulleted-list">
        <li>
          <a href="https://duckduckgo.com/?q=site%3Acdn.openai.com&amp;t=hk&amp;ia=web">https://duckduckgo.com/?q=site%3Acdn.openai.com&amp;t=hk&amp;ia=web</a>
        </li>
      </ul>
      <h1 id="03ec2447-0b3f-4c03-a4ca-f7ed7c18f891" class="">RelevantnÃ­ diskuze</h1>
      <ul id="5714e175-af5f-4d9c-80fc-e788bd46da6d" class="bulleted-list">
        <li>
          <a href="https://www.abclinuxu.cz/blog/bystroushaak/2020/8/gpt-3/diskuse">abclinuxu</a>
        </li>
      </ul>
    </div>
  </article>
  <div class="corner-ribbon top-right red">
    <a href="https://www.patreon.com/bePatron?u=2618881">Become a Patron</a>
  </div><a class="twitter-share-button" id="twitter_button" href="#"><img src="../../../tweet_button.svg"></a>
  <div id="sidebar_bottom">
    <span><a href="https://blog.rfox.eu/atom_cz.xml"><img style="width: 3em;" src="../../../rss_icon.png"></a> &nbsp; &nbsp; <a href="https://twitter.com/Bystroushaak"><img style="width: 3em;" src="../../../twitter_icon.png"></a></span>
    <div id="last_five_bottom">
      <h3>New posts</h3>
      <ul>
        <li>
          <a href="../GPT-3.html" title="GPT-3">GPT-3</a>
        </li>
        <li>
          <a href="https://blog.rfox.eu/atom_cz.xml">VytvoÅ™en novÃ½ Atom (RSS) feed pro Äeskou sekci</a>
        </li>
      </ul>
    </div>
    <div id="links_from_other_pages">
      <h3>Links to this page:</h3>
      <ul>
        <li>
          <a href="../../../Zmeny.html" title="ZmÄ›ny">ZmÄ›ny</a>
        </li>
        <li>
          <a href="../../../Zmeny/Poslednich_nekolik_tydnu_rozechviva_vlny_mych_socialnich_siti_fenomen.html" title="PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno pÅ™edstavenÃ½ druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­.">PoslednÃ­ch nÄ›kolik tÃ½dnÅ¯ rozechvÃ­vÃ¡ vlny mÃ½ch sociÃ¡lnÃ­ch sÃ­tÃ­ fenomÃ©n GPT-3. JednÃ¡ se o nedÃ¡vno pÅ™edstavenÃ½ druh strojovÃ©ho uÄenÃ­, vytrÃ©novanÃ½ spoleÄnostÃ­ OpenAI na rekordnÃ­m mnoÅ¾stvÃ­ dat. A zatÃ­mco se jednÃ¡ jen o jazykovÃ½ model, kterÃ½ mÃ¡ za Ãºkol predikovat dalÅ¡Ã­ token ve vÄ›tÄ›, vÃ½sledky a moÅ¾nosti vyuÅ¾itÃ­ jsou mÃ­sty dech-beroucÃ­.</a>
        </li>
        <li>
          <a href="../../../en/Weekly_updates/Newsletter_2020-09-12_Waves_of_productivity.html" title="Newsletter 2020-09-12; Waves of productivity">Newsletter 2020-09-12; Waves of productivity</a>
        </li>
      </ul>
    </div>
    <div>
      <h3>Blog categories</h3>
      <ul class="no_icon">
        <li>
          <a href="../../Programovani.html" title="ProgramovÃ¡nÃ­">ğŸ“‚ ProgramovÃ¡nÃ­</a>
        </li>
        <li>
          <a href="../../Knihy.html" title="Knihy">ğŸ“‚ Knihy</a>
        </li>
        <li>
          <a href="../../Predstaveni.html" title="PÅ™edstavenÃ­">ğŸ“‚ PÅ™edstavenÃ­</a>
        </li>
        <li>
          <a href="../../Povidky.html" title="PovÃ­dky">ğŸ“‚ PovÃ­dky</a>
        </li>
        <li>
          <a href="../../Hrbitov.html" title="HÅ™bitov">ğŸ“‚ HÅ™bitov</a>
        </li>
        <li>
          <a href="../../Ostatni.html" title="OstatnÃ­">ğŸ“‚ OstatnÃ­</a>
        </li>
        <li>
          <a href="../../Abclinuxu.html" title="Abclinuxu">ğŸ“‚ Abclinuxu</a>
        </li>
        <li>
          <a href="../../index.html" title="Czech section">ğŸ“‚ Czech section</a>
        </li>
      </ul>
    </div>
  </div>
</body>
</html>
